# apps/run_ate_estimation.py

import numpy as np
import pandas as pd
from  sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from libs.causal.dr_ate import DoubleRobustATE
from apps.prepare_push_data import PushDataPreparator
import matplotlib.pyplot as plt
import seaborn as sns

def main():
    """
    Pushé…ä¿¡ã®ATEæ¨å®šãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
    """
    
    print("="*60)
    print("ğŸš€ Pushé…ä¿¡åŠ¹æœã®ãƒ€ãƒ–ãƒ«ãƒ­ãƒã‚¹ãƒˆæ¨å®š")
    print("   Google/Meta/NASAæ°´æº–ã®å› æœæ¨è«–å®Ÿè£…")
    print("="*60)
    
    # 1. ãƒ‡ãƒ¼ã‚¿æº–å‚™
    print("\nğŸ“¥ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    preparator = PushDataPreparator()
    df = preparator.create_analysis_dataset('2024-01-01', '2024-01-31')
    
    # ç‰¹å¾´é‡ã¨ã‚¢ã‚¦ãƒˆã‚«ãƒ ã®æº–å‚™
    feature_cols = ['age', 'gender_male', 'recency_days', 
                   'frequency_30d', 'monetary_30d', 'prev_week_sales']
    
    X = df[feature_cols].fillna(0).values
    T = df['treated'].values
    Y = df['outcome_sales'].values
    
    # ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åŒ–ï¼ˆé‡è¦ï¼šåæŸã‚’è‰¯ãã™ã‚‹ï¼‰
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # 2. DRæ¨å®š
    print("\nğŸ”¬ ãƒ€ãƒ–ãƒ«ãƒ­ãƒã‚¹ãƒˆæ¨å®šã‚’å®Ÿè¡Œä¸­...")
    
    # ãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆMetaã®å®Ÿè£…ã«æº–æ‹ ï¼‰
    outcome_model = RandomForestRegressor(
        n_estimators=100,
        max_depth=10,
        min_samples_leaf=50,  # éå­¦ç¿’é˜²æ­¢
        random_state=42
    )
    
    propensity_model = RandomForestClassifier(
        n_estimators=100,
        max_depth=8,
        min_samples_leaf=100,  # ãƒãƒ©ãƒ³ã‚¹é‡è¦–
        random_state=42
    )
    
    # DRæ¨å®šå™¨
    dr_estimator = DoubleRobustATE(
        outcome_model=outcome_model,
        propensity_model=propensity_model,
        trim_threshold=0.05  # 5%ã§ãƒˆãƒªãƒŸãƒ³ã‚°
    )
    
    # å­¦ç¿’ã¨æ¨å®š
    dr_estimator.fit(X_scaled, T, Y)
    result = dr_estimator.ate_result
    
    # 3. çµæœè¡¨ç¤º
    print("\n" + "="*60)
    print("ğŸ“Š æ¨å®šçµæœ")
    print("="*60)
    print(f"å¹³å‡å‡¦ç½®åŠ¹æœï¼ˆATEï¼‰: {result.ate:,.2f} å††")
    print(f"æ¨™æº–èª¤å·®: {result.se:,.2f} å††")
    print(f"95%ä¿¡é ¼åŒºé–“: [{result.ci_lower:,.2f}, {result.ci_upper:,.2f}]")
    print(f"çµ±è¨ˆçš„æœ‰æ„æ€§: {'âœ… æœ‰æ„' if result.ci_lower > 0 else 'âŒ éæœ‰æ„'}")
    
    # ROIè¨ˆç®—ï¼ˆãƒ“ã‚¸ãƒã‚¹æŒ‡æ¨™ï¼‰
    cost_per_push = 10  # Pushé…ä¿¡ã‚³ã‚¹ãƒˆï¼ˆä»®ï¼‰
    roi = (result.ate - cost_per_push) / cost_per_push * 100
    print(f"\nğŸ’° ROI: {roi:.1f}%")
    
    # 4. è¨ºæ–­
    diagnostics = dr_estimator.get_diagnostics()
    print("\n" + "="*60)
    print("ğŸ” è¨ºæ–­çµ±è¨ˆé‡")
    print("="*60)
    print(f"å‚¾å‘ã‚¹ã‚³ã‚¢åˆ†å¸ƒ:")
    print(f"  - æœ€å°å€¤: {diagnostics['propensity_score']['min']:.3f}")
    print(f"  - æœ€å¤§å€¤: {diagnostics['propensity_score']['max']:.3f}")
    print(f"  - æ¥µç«¯ãªå€¤(<0.1 or >0.9): {diagnostics['propensity_score']['extreme_low']*100:.1f}%")
    print(f"æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º:")
    print(f"  - å‡¦ç½®ç¾¤: {diagnostics['ess_treated']:.0f}")
    print(f"  - çµ±åˆ¶ç¾¤: {diagnostics['ess_control']:.0f}")
    print(f"æœ€å¤§é‡ã¿: {diagnostics['max_weight']:.1f}")
    
    # 5. å¯è¦–åŒ–
    create_diagnostic_plots(dr_estimator, result)
    
    # 6. çµæœã®ä¿å­˜
    save_results(result, diagnostics)
    
    print("\nâœ¨ åˆ†æå®Œäº†ï¼")
    print("ğŸ“ çµæœã¯ docs/ ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")

def create_diagnostic_plots(estimator, result):
    """è¨ºæ–­ãƒ—ãƒ­ãƒƒãƒˆã®ä½œæˆï¼ˆGoogleã‚¹ã‚¿ã‚¤ãƒ«ï¼‰"""
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # 1. å‚¾å‘ã‚¹ã‚³ã‚¢åˆ†å¸ƒ
    ax = axes[0, 0]
    ax.hist(estimator.e_hat[estimator.T == 1], alpha=0.5, label='Treated', bins=30)
    ax.hist(estimator.e_hat[estimator.T == 0], alpha=0.5, label='Control', bins=30)
    ax.set_xlabel('Propensity Score')
    ax.set_ylabel('Frequency')
    ax.set_title('Propensity Score Distribution')
    ax.legend()
    
    # 2. å½±éŸ¿é–¢æ•°ã®åˆ†å¸ƒ
    ax = axes[0, 1]
    ax.hist(result.influence_function, bins=50, edgecolor='black')
    ax.axvline(result.ate, color='red', linestyle='--', label=f'ATE={result.ate:.2f}')
    ax.set_xlabel('Influence Function')
    ax.set_ylabel('Frequency')
    ax.set_title('Influence Function Distribution')
    ax.legend()
    
    # 3. Love Plotï¼ˆå…±å¤‰é‡ãƒãƒ©ãƒ³ã‚¹ï¼‰
    # ... å®Ÿè£…çœç•¥ ...
    
    plt.tight_layout()
    plt.savefig('docs/diagnostic_plots.png', dpi=150)
    plt.close()

def save_results(result, diagnostics):
    """çµæœã®ä¿å­˜"""
    
    # DataFrameã«ã¾ã¨ã‚ã‚‹
    results_df = pd.DataFrame({
        'metric': ['ATE', 'SE', 'CI_lower', 'CI_upper', 'N_treated', 'N_control'],
        'value': [result.ate, result.se, result.ci_lower, 
                 result.ci_upper, result.n_treated, result.n_control]
    })
    
    # Parquetå½¢å¼ã§ä¿å­˜ï¼ˆé«˜é€Ÿãƒ»åœ§ç¸®ï¼‰
    results_df.to_parquet('data/processed/ate_results.parquet')
    
    # è¨ºæ–­çµæœã‚‚ä¿å­˜
    diag_df = pd.DataFrame([diagnostics])
    diag_df.to_parquet('data/processed/diagnostics.parquet')

if __name__ == "__main__":
    main()
