# apps/run_ate_analysis.py

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# scikit-learnã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆæ˜ç¤ºçš„ã«ãƒ•ãƒ«ãƒãƒ¼ãƒ ä½¿ç”¨ï¼‰
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# è‡ªä½œãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from libs.causal.dr_ate import DoubleRobustATE

def load_data(data_path: str = 'data/sample_data.parquet') -> pd.DataFrame:
    """
    ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    
    Args:
        data_path: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    
    Returns:
        DataFrame
    """
    if not os.path.exists(data_path):
        print(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {data_path}")
        print("ã¾ãšä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
        print("  python apps/generate_sample_data.py")
        sys.exit(1)
    
    print(f"ğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {data_path}")
    
    # æ‹¡å¼µå­ã«ã‚ˆã£ã¦èª­ã¿è¾¼ã¿æ–¹æ³•ã‚’å¤‰æ›´
    if data_path.endswith('.parquet'):
        df = pd.read_parquet(data_path)
    elif data_path.endswith('.csv'):
        df = pd.read_csv(data_path)
    else:
        raise ValueError(f"Unsupported file format: {data_path}")
    
    print(f"âœ… {len(df):,} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    
    return df

def prepare_features(df: pd.DataFrame) -> tuple:
    """
    ç‰¹å¾´é‡ã®æº–å‚™
    
    Args:
        df: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    
    Returns:
        X, T, Y ã® tuple
    """
    # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ 
    feature_cols = [
        'age', 'gender_male', 'recency_days', 
        'frequency_30d', 'monetary_30d', 
        'prev_week_sales', 'prev_month_sales'
    ]
    
    # æ¬ æå€¤ã®ç¢ºèª
    missing = df[feature_cols].isnull().sum()
    if missing.sum() > 0:
        print("\nâš ï¸ æ¬ æå€¤ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ:")
        print(missing[missing > 0])
        print("æ¬ æå€¤ã‚’ä¸­å¤®å€¤ã§è£œå®Œã—ã¾ã™...")
        
        for col in feature_cols:
            if df[col].isnull().sum() > 0:
                median_val = df[col].median()
                df[col].fillna(median_val, inplace=True)
    
    X = df[feature_cols].values
    T = df['treated'].values
    Y = df['outcome_sales'].values
    
    print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶:")
    print(f"   X (ç‰¹å¾´é‡): {X.shape}")
    print(f"   T (å‡¦ç½®): {T.shape}")
    print(f"   Y (çµæœ): {Y.shape}")
    
    return X, T, Y

def create_diagnostic_plots(dr_estimator, save_dir='docs'):
    """
    è¨ºæ–­ãƒ—ãƒ­ãƒƒãƒˆã®ä½œæˆ
    
    Args:
        dr_estimator: å­¦ç¿’æ¸ˆã¿ã®DRæ¨å®šå™¨
        save_dir: ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    """
    os.makedirs(save_dir, exist_ok=True)
    
    # ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
    plt.style.use('seaborn-v0_8-darkgrid')
    sns.set_palette("husl")
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    
    # 1. å‚¾å‘ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒ
    ax = axes[0, 0]
    ps = dr_estimator.e_hat
    treated_mask = dr_estimator.ate_result.n_treated
    
    ax.hist(ps[dr_estimator.ate_result.n_treated], 
            alpha=0.6, label='Treated', bins=30, color='orange')
    ax.hist(ps[~dr_estimator.ate_result.n_treated], 
            alpha=0.6, label='Control', bins=30, color='blue')
    ax.set_xlabel('Propensity Score')
    ax.set_ylabel('Frequency')
    ax.set_title('Propensity Score Distribution')
    ax.legend()
    ax.axvline(0.5, color='red', linestyle='--', alpha=0.5)
    
    # 2. å½±éŸ¿é–¢æ•°ã®åˆ†å¸ƒ
    ax = axes[0, 1]
    psi = dr_estimator.ate_result.influence_function
    ate = dr_estimator.ate_result.ate
    
    ax.hist(psi, bins=50, edgecolor='black', alpha=0.7, color='green')
    ax.axvline(ate, color='red', linestyle='--', 
               label=f'ATE={ate:.2f}', linewidth=2)
    ax.set_xlabel('Influence Function Value')
    ax.set_ylabel('Frequency')
    ax.set_title('Influence Function Distribution')
    ax.legend()
    
    # 3. äºˆæ¸¬å€¤ vs å®Ÿæ¸¬å€¤ï¼ˆå‡¦ç½®ç¾¤ï¼‰
    ax = axes[0, 2]
    mu_1 = dr_estimator.mu_1_hat
    ax.scatter(mu_1[:100], psi[:100], alpha=0.5, s=20)
    ax.set_xlabel('Predicted Outcome (Treated)')
    ax.set_ylabel('Influence Function')
    ax.set_title('Prediction vs Influence (Sample)')
    
    # 4. å‚¾å‘ã‚¹ã‚³ã‚¢ vs å½±éŸ¿é–¢æ•°
    ax = axes[1, 0]
    ax.scatter(ps, psi, alpha=0.3, s=10)
    ax.set_xlabel('Propensity Score')
    ax.set_ylabel('Influence Function')
    ax.set_title('PS vs Influence Function')
    ax.axvline(0.5, color='red', linestyle='--', alpha=0.3)
    ax.axhline(ate, color='red', linestyle='--', alpha=0.3)
    
    # 5. é‡ã¿ã®åˆ†å¸ƒ
    ax = axes[1, 1]
    weights = np.where(dr_estimator.ate_result.n_treated, 
                      1/ps, 1/(1-ps))
    ax.hist(np.clip(weights, 0, 20), bins=50, edgecolor='black', alpha=0.7)
    ax.set_xlabel('IPW Weights (clipped at 20)')
    ax.set_ylabel('Frequency')
    ax.set_title('Weight Distribution')
    
    # 6. è¨ºæ–­çµ±è¨ˆé‡ã®ãƒ†ã‚­ã‚¹ãƒˆ
    ax = axes[1, 2]
    ax.axis('off')
    
    diagnostics = dr_estimator.get_diagnostics()
    
    stats_text = f"""
    Diagnostic Statistics
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    Propensity Score:
    â€¢ Range: [{diagnostics['propensity_score']['min']:.3f}, 
             {diagnostics['propensity_score']['max']:.3f}]
    â€¢ Mean: {diagnostics['propensity_score']['mean']:.3f}
    â€¢ Extreme (<0.1 or >0.9): 
      {diagnostics['propensity_score']['extreme_low']*100:.1f}%
    
    Effective Sample Size:
    â€¢ ESS Ratio: {diagnostics['ess_ratio']:.3f}
    â€¢ Max Weight: {diagnostics['max_weight']:.1f}
    
    Quality Check:
    {'âœ… Good balance' if diagnostics['ess_ratio'] > 0.2 else 'âš ï¸ Poor balance'}
    {'âœ… Stable weights' if diagnostics['max_weight'] < 50 else 'âš ï¸ Extreme weights'}
    """
    
    ax.text(0.1, 0.5, stats_text, fontsize=10, 
            verticalalignment='center', fontfamily='monospace')
    
    plt.suptitle('Double Robust ATE - Diagnostic Report', 
                 fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    # ä¿å­˜
    plot_path = os.path.join(save_dir, 'dr_diagnostics.png')
    plt.savefig(plot_path, dpi=150, bbox_inches='tight')
    print(f"\nğŸ“Š è¨ºæ–­ãƒ—ãƒ­ãƒƒãƒˆä¿å­˜: {plot_path}")
    
    plt.show()

def main():
    """
    ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
    """
    print("\n" + "="*80)
    print("ğŸš€ Pushé…ä¿¡åŠ¹æœã®ãƒ€ãƒ–ãƒ«ãƒ­ãƒã‚¹ãƒˆæ¨å®š")
    print("   ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸåˆ†æ")
    print("="*80)
    
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\n" + "â”€"*60)
    print("ğŸ“¥ Phase 1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
    print("â”€"*60)
    
    df = load_data('data/sample_data.parquet')
    
    # ãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦è¡¨ç¤º
    print("\nãƒ‡ãƒ¼ã‚¿æ¦‚è¦:")
    print(df.info())
    
    print("\nåŸºæœ¬çµ±è¨ˆ:")
    print(df[['treated', 'outcome_sales', 'age', 'frequency_30d']].describe())
    
    # 2. ç‰¹å¾´é‡æº–å‚™
    print("\n" + "â”€"*60)
    print("ğŸ”§ Phase 2: ç‰¹å¾´é‡æº–å‚™")
    print("â”€"*60)
    
    X, T, Y = prepare_features(df)
    
    # æ¨™æº–åŒ–
    print("\nğŸ“ ç‰¹å¾´é‡ã‚’æ¨™æº–åŒ–ä¸­...")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # 3. ãƒ¢ãƒ‡ãƒ«è¨­å®š
    print("\n" + "â”€"*60)
    print("âš™ï¸ Phase 3: ãƒ¢ãƒ‡ãƒ«è¨­å®š")
    print("â”€"*60)
    
    # çµæœäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ï¼ˆRandom Forestï¼‰
    outcome_model = RandomForestRegressor(
        n_estimators=100,      # æœ¨ã®æ•°
        max_depth=10,          # æœ¨ã®æ·±ã•
        min_samples_leaf=50,   # è‘‰ã®æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
        random_state=42,       # ä¹±æ•°ã‚·ãƒ¼ãƒ‰
        n_jobs=-1              # ä¸¦åˆ—å‡¦ç†
    )
    
    # å‚¾å‘ã‚¹ã‚³ã‚¢ãƒ¢ãƒ‡ãƒ«ï¼ˆRandom Foreståˆ†é¡å™¨ï¼‰
    propensity_model = RandomForestClassifier(
        n_estimators=100,
        max_depth=8,
        min_samples_leaf=100,
        random_state=42,
        n_jobs=-1
    )
    
    print("âœ… ãƒ¢ãƒ‡ãƒ«è¨­å®šå®Œäº†:")
    print(f"   çµæœãƒ¢ãƒ‡ãƒ«: {outcome_model.__class__.__name__}")
    print(f"   å‚¾å‘ã‚¹ã‚³ã‚¢ãƒ¢ãƒ‡ãƒ«: {propensity_model.__class__.__name__}")
    
    # 4. DRæ¨å®š
    print("\n" + "â”€"*60)
    print("ğŸ§® Phase 4: ãƒ€ãƒ–ãƒ«ãƒ­ãƒã‚¹ãƒˆæ¨å®š")
    print("â”€"*60)
    
    dr_estimator = DoubleRobustATE(
        outcome_model=outcome_model,
        propensity_model=propensity_model,
        trim_threshold=0.02,  # 2%ã§ãƒˆãƒªãƒŸãƒ³ã‚°
        verbose=True
    )
    
    # å­¦ç¿’ã¨æ¨å®š
    dr_estimator.fit(X_scaled, T, Y)
    
    # 5. çµæœã®è§£é‡ˆ
    print("\n" + "â”€"*60)
    print("ğŸ“ˆ Phase 5: çµæœã®è§£é‡ˆ")
    print("â”€"*60)
    
    result = dr_estimator.ate_result
    
    # åŠ¹æœã‚µã‚¤ã‚ºã®è¨ˆç®—
    control_mean = Y[T == 0].mean()
    relative_effect = (result.ate / control_mean) * 100
    
    print(f"\nğŸ’¡ ãƒ“ã‚¸ãƒã‚¹çš„è§£é‡ˆ:")
    print(f"   çµ±åˆ¶ç¾¤ã®å¹³å‡è³¼è²·é¡: {control_mean:,.2f} å††")
    print(f"   å‡¦ç½®åŠ¹æœï¼ˆçµ¶å¯¾å€¤ï¼‰: {result.ate:,.2f} å††")
    print(f"   å‡¦ç½®åŠ¹æœï¼ˆç›¸å¯¾å€¤ï¼‰: {relative_effect:.1f}%")
    
    # ROIè¨ˆç®—ï¼ˆä»®ã®é…ä¿¡ã‚³ã‚¹ãƒˆï¼‰
    cost_per_push = 10  # Pushé…ä¿¡1ä»¶ã‚ãŸã‚Šã®ã‚³ã‚¹ãƒˆ
    roi = (result.ate - cost_per_push) / cost_per_push * 100
    
    print(f"\nğŸ’° ROIåˆ†æ:")
    print(f"   é…ä¿¡ã‚³ã‚¹ãƒˆ: {cost_per_push} å††/ä»¶")
    print(f"   ç´”åˆ©ç›Š: {result.ate - cost_per_push:,.2f} å††/ä»¶")
    print(f"   ROI: {roi:.1f}%")
    
    if roi > 0:
        print(f"   â†’ âœ… Pushé…ä¿¡ã¯è²»ç”¨å¯¾åŠ¹æœãŒé«˜ã„")
    else:
        print(f"   â†’ âŒ Pushé…ä¿¡ã¯è²»ç”¨å¯¾åŠ¹æœãŒä½ã„")
    
    # 6. è¨ºæ–­
    print("\n" + "â”€"*60)
    print("ğŸ” Phase 6: è¨ºæ–­ã¨å¯è¦–åŒ–")
    print("â”€"*60)
    
    diagnostics = dr_estimator.get_diagnostics()
    
    # å“è³ªãƒã‚§ãƒƒã‚¯
    quality_checks = {
        'ESSæ¯” > 0.2': diagnostics['ess_ratio'] > 0.2,
        'æœ€å¤§é‡ã¿ < 50': diagnostics['max_weight'] < 50,
        'æ¥µç«¯ãªPS < 10%': (diagnostics['propensity_score']['extreme_low'] + 
                          diagnostics['propensity_score']['extreme_high']) < 0.1
    }
    
    print("\nå“è³ªãƒã‚§ãƒƒã‚¯:")
    for check, passed in quality_checks.items():
        status = "âœ…" if passed else "âŒ"
        print(f"   {status} {check}")
    
    # è¨ºæ–­ãƒ—ãƒ­ãƒƒãƒˆ
    create_diagnostic_plots(dr_estimator)
    
    # 7. çµæœã®ä¿å­˜
    print("\n" + "â”€"*60)
    print("ğŸ’¾ Phase 7: çµæœã®ä¿å­˜")
    print("â”€"*60)
    
    # çµæœã‚’DataFrameã«æ•´ç†
    results_df = pd.DataFrame({
        'metric': ['ATE', 'SE', 'CI_lower', 'CI_upper', 
                  'Relative_effect_%', 'ROI_%', 'N_treated', 'N_control'],
        'value': [result.ate, result.se, result.ci_lower, result.ci_upper,
                 relative_effect, roi, result.n_treated, result.n_control]
    })
    
    # ä¿å­˜
    os.makedirs('results', exist_ok=True)
    results_path = 'results/dr_ate_results.csv'
    results_df.to_csv(results_path, index=False)
    print(f"âœ… çµæœã‚’ä¿å­˜: {results_path}")
    
    # çœŸå€¤ã¨ã®æ¯”è¼ƒï¼ˆã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã®ã¿ï¼‰
    if os.path.exists('data/sample_data_with_truth.parquet'):
        print("\n" + "â”€"*60)
        print("ğŸ¯ çœŸå€¤ã¨ã®æ¯”è¼ƒï¼ˆæ¤œè¨¼ç”¨ï¼‰")
        print("â”€"*60)
        
        df_truth = pd.read_parquet('data/sample_data_with_truth.parquet')
        true_ate = df_truth['_true_effect'].mean()
        
        print(f"   çœŸã®ATE: {true_ate:,.2f} å††")
        print(f"   æ¨å®šATE: {result.ate:,.2f} å††")
        print(f"   æ¨å®šèª¤å·®: {abs(result.ate - true_ate):,.2f} å††")
        print(f"   ç›¸å¯¾èª¤å·®: {abs(result.ate - true_ate) / true_ate * 100:.1f}%")
        
        # çœŸå€¤ãŒä¿¡é ¼åŒºé–“ã«å«ã¾ã‚Œã‚‹ã‹
        if result.ci_lower <= true_ate <= result.ci_upper:
            print(f"   âœ… çœŸå€¤ã¯95%ä¿¡é ¼åŒºé–“ã«å«ã¾ã‚Œã‚‹")
        else:
            print(f"   âŒ çœŸå€¤ã¯95%ä¿¡é ¼åŒºé–“ã«å«ã¾ã‚Œãªã„")
    
    print("\n" + "="*80)
    print("âœ¨ åˆ†æå®Œäº†ï¼")
    print("="*80)

if __name__ == "__main__":
    main()
