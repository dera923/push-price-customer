import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from typing import Tuple, Optional, Dict

class StabilizedIPWEstimator:
    """
    NASA JPLæ¨™æº–æº–æ‹ ã®Stabilized IPWå®Ÿè£…
    
    ç†è«–åŸºç›¤:
    - Potential Outcomes Framework (Rubin, 1974)
    - Propensity Score Matching (Rosenbaum & Rubin, 1983)
    - Stabilized Weights (Robins et al., 2000)
    
    Google/Meta/NASAã§ã®å®Ÿç”¨ä¾‹:
    - Google: åºƒå‘Šé…ä¿¡ã®å› æœåŠ¹æœæ¸¬å®š
    - Meta: ãƒ•ã‚£ãƒ¼ãƒ‰æ”¹å¤‰ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ã¸ã®å½±éŸ¿
    - NASA: ã‚·ã‚¹ãƒ†ãƒ å¤‰æ›´ã®æ©Ÿå™¨æ€§èƒ½ã¸ã®å› æœåŠ¹æœ
    """
    
    def __init__(self, 
                 trim_threshold: float = 0.01,
                 stabilize_weights: bool = True,
                 clip_weights: bool = True,
                 max_weight: float = 100.0,
                 random_state: int = 42):
        """
        Parameters:
        - trim_threshold: PS < threshold or PS > (1-threshold)ã‚’é™¤å¤–
        - stabilize_weights: Stabilized weightsã‚’ä½¿ç”¨ã™ã‚‹ã‹
        - clip_weights: æ¥µç«¯ãªé‡ã¿ã‚’ã‚¯ãƒªãƒƒãƒ—ã™ã‚‹ã‹
        - max_weight: é‡ã¿ã®ä¸Šé™å€¤
        """
        self.trim_threshold = trim_threshold
        self.stabilize_weights = stabilize_weights
        self.clip_weights = clip_weights
        self.max_weight = max_weight
        self.random_state = random_state
        
        # è¨ºæ–­æƒ…å ±ã‚’ä¿å­˜
        self.diagnostics = {}
        self.ps_model = None
        
    def fit_propensity_score(self, X: np.ndarray, T: np.ndarray) -> np.ndarray:
        """
        å‚¾å‘ã‚¹ã‚³ã‚¢ã®æ¨å®š
        
        NASAæ¨™æº–: Cross-validationã§æ€§èƒ½è©•ä¾¡ã‚’å¿…é ˆã¨ã™ã‚‹
        """
        # ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åŒ–ï¼ˆæ•°å€¤å®‰å®šæ€§ã®ãŸã‚ï¼‰
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X)
        
        # Logistic Regressionã§ã®å‚¾å‘ã‚¹ã‚³ã‚¢æ¨å®š
        self.ps_model = LogisticRegression(
            random_state=self.random_state,
            max_iter=1000,
            penalty='l2',  # éå­¦ç¿’é˜²æ­¢
            C=1.0
        )
        
        # Cross-validationæ€§èƒ½è©•ä¾¡
        cv_scores = cross_val_score(
            self.ps_model, X_scaled, T, 
            cv=5, scoring='roc_auc'
        )
        
        # æ€§èƒ½ãŒä½ã™ãã‚‹å ´åˆã¯è­¦å‘Š
        if cv_scores.mean() < 0.6:
            warnings.warn(
                f"å‚¾å‘ã‚¹ã‚³ã‚¢ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ãŒä½ã„: AUC={cv_scores.mean():.3f}\n"
                f"Unconfoundednessä»®å®šãŒæˆç«‹ã—ãªã„å¯èƒ½æ€§"
            )
        
        self.ps_model.fit(X_scaled, T)
        propensity_scores = self.ps_model.predict_proba(X_scaled)[:, 1]
        
        # è¨ºæ–­æƒ…å ±ã®è¨˜éŒ²
        self.diagnostics['ps_cv_auc'] = cv_scores.mean()
        self.diagnostics['ps_cv_std'] = cv_scores.std()
        self.diagnostics['ps_min'] = propensity_scores.min()
        self.diagnostics['ps_max'] = propensity_scores.max()
        
        return propensity_scores
    
    def compute_stabilized_weights(self, 
                                 T: np.ndarray, 
                                 propensity_scores: np.ndarray) -> np.ndarray:
        """
        Stabilized Weightsã®è¨ˆç®—
        
        ç†è«–:
        SW_i = [T_i * P(T=1) / e(X_i)] + [(1-T_i) * P(T=0) / (1-e(X_i))]
        
        é€šå¸¸ã®IPWã¨ã®é•ã„:
        - åˆ†å­ã«å‘¨è¾ºç¢ºç‡ã‚’ä¹—ã˜ã‚‹ã“ã¨ã§å®‰å®šåŒ–
        - Effective Sample Sizeã®æ”¹å–„
        - åˆ†æ•£ã®å‰Šæ¸›
        """
        # Trimmingã®å®Ÿè¡Œï¼ˆæ¥µç«¯å€¤ã®é™¤å¤–ï¼‰
        valid_mask = (
            (propensity_scores >= self.trim_threshold) & 
            (propensity_scores <= 1 - self.trim_threshold)
        )
        
        if valid_mask.sum() < len(T) * 0.8:
            warnings.warn(
                f"Trimmingã«ã‚ˆã‚Š{(~valid_mask).sum()}ã‚µãƒ³ãƒ—ãƒ«"
                f"({(~valid_mask).mean():.1%})ãŒé™¤å¤–ã•ã‚Œã¾ã—ãŸã€‚"
                f"å…±é€šã‚µãƒãƒ¼ãƒˆãŒä¸ååˆ†ãªå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
            )
        
        # å‘¨è¾ºç¢ºç‡ã®è¨ˆç®—
        p_treat = T[valid_mask].mean()  # P(T=1)
        
        if self.stabilize_weights:
            # Stabilized Weights
            weights = np.zeros(len(T))
            weights[valid_mask] = (
                T[valid_mask] * p_treat / propensity_scores[valid_mask] +
                (1 - T[valid_mask]) * (1 - p_treat) / (1 - propensity_scores[valid_mask])
            )
        else:
            # é€šå¸¸ã®IPW weights
            weights = np.zeros(len(T))
            weights[valid_mask] = (
                T[valid_mask] / propensity_scores[valid_mask] +
                (1 - T[valid_mask]) / (1 - propensity_scores[valid_mask])
            )
        
        # æ¥µç«¯ãªé‡ã¿ã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
        if self.clip_weights:
            weights = np.clip(weights, 0, self.max_weight)
            
        # é‡ã¿ã®æ­£è¦åŒ–ï¼ˆåˆè¨ˆãŒã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã«ãªã‚‹ã‚ˆã†ã«ï¼‰
        if weights.sum() > 0:
            weights = weights * len(T) / weights.sum()
        
        # è¨ºæ–­æƒ…å ±ã®è¨˜éŒ²
        self.diagnostics.update({
            'n_trimmed': (~valid_mask).sum(),
            'trim_rate': (~valid_mask).mean(),
            'weight_min': weights[weights > 0].min() if (weights > 0).any() else 0,
            'weight_max': weights.max(),
            'weight_mean': weights[weights > 0].mean() if (weights > 0).any() else 0,
            'effective_sample_size': self._compute_ess(weights)
        })
        
        return weights, valid_mask
    
    def _compute_ess(self, weights: np.ndarray) -> float:
        """
        Effective Sample Size (ESS) ã®è¨ˆç®—
        
        ESS = (Î£w_i)Â² / Î£w_iÂ²
        
        è§£é‡ˆ:
        - ESSãŒå…ƒã®ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã«è¿‘ã„: é‡ã¿ãŒå‡ä¸€ã§è‰¯å¥½
        - ESSãŒå°ã•ã„: ä¸€éƒ¨ã®ã‚µãƒ³ãƒ—ãƒ«ã«é‡ã¿ãŒé›†ä¸­ï¼ˆå•é¡Œï¼‰
        """
        if weights.sum() == 0:
            return 0
        return (weights.sum() ** 2) / (weights ** 2).sum()
    
    def estimate_ate(self, 
                    Y: np.ndarray, 
                    T: np.ndarray, 
                    X: np.ndarray) -> Dict:
        """
        Average Treatment Effect (ATE) ã®æ¨å®š
        
        Returns:
        - ate: æ¨å®šã•ã‚ŒãŸATE
        - ate_std: æ¨™æº–èª¤å·®
        - confidence_interval: 95%ä¿¡é ¼åŒºé–“
        - diagnostics: è¨ºæ–­æƒ…å ±
        """
        # ã‚¹ãƒ†ãƒƒãƒ—1: å‚¾å‘ã‚¹ã‚³ã‚¢ã®æ¨å®š
        propensity_scores = self.fit_propensity_score(X, T)
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: Stabilized Weightsã®è¨ˆç®—
        weights, valid_mask = self.compute_stabilized_weights(T, propensity_scores)
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: ATEã®è¨ˆç®—ï¼ˆæœ‰åŠ¹ãªã‚µãƒ³ãƒ—ãƒ«ã®ã¿ä½¿ç”¨ï¼‰
        Y_valid = Y[valid_mask]
        T_valid = T[valid_mask]
        weights_valid = weights[valid_mask]
        
        if weights_valid.sum() == 0:
            raise ValueError("æœ‰åŠ¹ãªé‡ã¿ãŒ0ã§ã™ã€‚Trimmingã®é–¾å€¤ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
        
        # å‡¦ç½®ç¾¤ã¨å¯¾ç…§ç¾¤ã®åŠ é‡å¹³å‡
        treated_outcome = np.average(
            Y_valid[T_valid == 1], 
            weights=weights_valid[T_valid == 1]
        ) if (T_valid == 1).any() else 0
        
        control_outcome = np.average(
            Y_valid[T_valid == 0], 
            weights=weights_valid[T_valid == 0]
        ) if (T_valid == 0).any() else 0
        
        ate = treated_outcome - control_outcome
        
        # ã‚¹ãƒ†ãƒƒãƒ—4: æ¨™æº–èª¤å·®ã®è¨ˆç®—ï¼ˆInfluence FunctionåŸºæº–ï¼‰
        ate_var = self._compute_ate_variance(Y_valid, T_valid, weights_valid, ate)
        ate_std = np.sqrt(ate_var / len(Y_valid))
        
        # ã‚¹ãƒ†ãƒƒãƒ—5: ä¿¡é ¼åŒºé–“ã®è¨ˆç®—
        ci_lower = ate - 1.96 * ate_std
        ci_upper = ate + 1.96 * ate_std
        
        # çµæœã®æ§‹ç¯‰
        results = {
            'ate': ate,
            'ate_std': ate_std,
            'confidence_interval': (ci_lower, ci_upper),
            'p_value': 2 * (1 - abs(ate / ate_std)),  # è¿‘ä¼¼çš„ãªpå€¤
            'diagnostics': self.diagnostics.copy()
        }
        
        return results
    
    def _compute_ate_variance(self, 
                            Y: np.ndarray, 
                            T: np.ndarray, 
                            weights: np.ndarray, 
                            ate: float) -> float:
        """
        IPWæ¨å®šé‡ã®åˆ†æ•£è¨ˆç®—ï¼ˆInfluence FunctionåŸºæº–ï¼‰
        
        ç†è«–:
        Var(ATE_IPW) = Var(Ï†_i) / n
        where Ï†_i = T_i * Y_i * w_i - (1-T_i) * Y_i * w_i - ATE
        """
        n = len(Y)
        
        # Influence functionã®è¨ˆç®—
        phi = np.zeros(n)
        
        # å‡¦ç½®ç¾¤ã¸ã®è²¢çŒ®
        treated_mask = (T == 1)
        if treated_mask.any():
            phi[treated_mask] += Y[treated_mask] * weights[treated_mask]
        
        # å¯¾ç…§ç¾¤ã¸ã®è²¢çŒ®
        control_mask = (T == 0)
        if control_mask.any():
            phi[control_mask] -= Y[control_mask] * weights[control_mask]
        
        # ATEã®å·®ã—å¼•ã
        phi -= ate
        
        # åˆ†æ•£ã®è¨ˆç®—
        variance = np.var(phi, ddof=1)
        return variance

def simulate_marketing_data(n: int = 10000, 
                          p_features: int = 50,
                          true_ate: float = 0.05,
                          selection_bias: float = 0.3,
                          random_state: int = 42) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æ–½ç­–ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    
    NASAæ¨™æº–: å…¨ã¦ã®å®Ÿè£…ã¯æ¤œè¨¼å¯èƒ½ãªã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã§é–‹å§‹
    
    Parameters:
    - n: ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º
    - p_features: ç‰¹å¾´é‡ã®æ•°
    - true_ate: çœŸã®ATEå€¤
    - selection_bias: é¸æŠãƒã‚¤ã‚¢ã‚¹ã®å¼·åº¦
    """
    np.random.seed(random_state)
    
    # é¡§å®¢ç‰¹å¾´é‡ã®ç”Ÿæˆï¼ˆå¹´é½¢ã€åå…¥ã€éå»ã®è³¼å…¥å±¥æ­´ãªã©ï¼‰
    X = np.random.randn(n, p_features)
    
    # é¸æŠãƒã‚¤ã‚¢ã‚¹: ç‰¹å®šã®é¡§å®¢ã«ã‚ˆã‚Šå¤šãPushé…ä¿¡ã•ã‚Œã‚‹
    propensity_logit = (
        selection_bias * X[:, 0] +  # å¹´é½¢ã«ã‚ˆã‚‹é¸æŠ
        0.2 * X[:, 1] +             # åå…¥ã«ã‚ˆã‚‹é¸æŠ
        0.1 * X[:, 2]               # è³¼å…¥å±¥æ­´ã«ã‚ˆã‚‹é¸æŠ
    )
    propensity_scores = 1 / (1 + np.exp(-propensity_logit))
    
    # å®Ÿéš›ã®å‡¦ç½®å‰²ã‚Šå½“ã¦
    T = np.random.binomial(1, propensity_scores)
    
    # ã‚¢ã‚¦ãƒˆã‚«ãƒ ï¼ˆç²—åˆ©ï¼‰ã®ç”Ÿæˆ
    # çœŸã®å› æœåŠ¹æœ + å…±å¤‰é‡ã®ç›´æ¥åŠ¹æœ + ãƒã‚¤ã‚º
    Y = (
        true_ate * T +                    # çœŸã®å‡¦ç½®åŠ¹æœ
        0.4 * X[:, 0] +                  # å¹´é½¢ã®ç›´æ¥åŠ¹æœ
        0.3 * X[:, 1] +                  # åå…¥ã®ç›´æ¥åŠ¹æœ
        np.random.normal(0, 1, n)        # ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¤ã‚º
    )
    
    return X, T, Y

# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
if __name__ == "__main__":
    # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
    X, T, Y = simulate_marketing_data(
        n=10000, 
        p_features=50, 
        true_ate=0.05,  # çœŸã®ATE = 5%ã®ç²—åˆ©å‘ä¸Š
        selection_bias=0.3
    )
    
    print("=== NASA/Googleæ°´æº– IPW Stabilized Weightså®Ÿè£… ===")
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(Y):,}ã‚µãƒ³ãƒ—ãƒ«")
    print(f"ç‰¹å¾´é‡æ•°: {X.shape[1]}")
    print(f"å‡¦ç½®å‰²ã‚Šå½“ã¦ç‡: {T.mean():.1%}")
    print(f"çœŸã®ATE: 5.0%")
    print()
    
    # IPWæ¨å®šã®å®Ÿè¡Œ
    estimator = StabilizedIPWEstimator(
        trim_threshold=0.01,
        stabilize_weights=True,
        clip_weights=True,
        max_weight=100.0
    )
    
    results = estimator.estimate_ate(Y, T, X)
    
    print("=== æ¨å®šçµæœ ===")
    print(f"æ¨å®šATE: {results['ate']:.4f} ({results['ate']:.1%})")
    print(f"æ¨™æº–èª¤å·®: {results['ate_std']:.4f}")
    print(f"95%ä¿¡é ¼åŒºé–“: [{results['confidence_interval'][0]:.4f}, {results['confidence_interval'][1]:.4f}]")
    print(f"På€¤: {results['p_value']:.4f}")
    print()
    
    print("=== è¨ºæ–­æƒ…å ± ===")
    diagnostics = results['diagnostics']
    print(f"å‚¾å‘ã‚¹ã‚³ã‚¢CV-AUC: {diagnostics['ps_cv_auc']:.3f} Â± {diagnostics['ps_cv_std']:.3f}")
    print(f"å‚¾å‘ã‚¹ã‚³ã‚¢ç¯„å›²: [{diagnostics['ps_min']:.3f}, {diagnostics['ps_max']:.3f}]")
    print(f"Trimmingç‡: {diagnostics['trim_rate']:.1%} ({diagnostics['n_trimmed']}ã‚µãƒ³ãƒ—ãƒ«é™¤å¤–)")
    print(f"é‡ã¿ç¯„å›²: [{diagnostics['weight_min']:.2f}, {diagnostics['weight_max']:.2f}]")
    print(f"Effective Sample Size: {diagnostics['effective_sample_size']:.0f} ({diagnostics['effective_sample_size']/len(Y):.1%})")
    
    # æˆåŠŸåˆ¤å®šï¼ˆStage 1ã®åŸºæº–ï¼‰
    bias = abs(results['ate'] - 0.05) / 0.05
    coverage = (results['confidence_interval'][0] <= 0.05 <= results['confidence_interval'][1])
    ess_ratio = diagnostics['effective_sample_size'] / len(Y)
    
    print()
    print("=== Stage 1 æˆåŠŸåŸºæº–ã®ç¢ºèª ===")
    print(f"ãƒã‚¤ã‚¢ã‚¹: {bias:.1%} {'âœ“' if bias < 0.05 else 'âœ—'} (åŸºæº–: <5%)")
    print(f"ä¿¡é ¼åŒºé–“ã‚«ãƒãƒ¬ãƒƒã‚¸: {'âœ“' if coverage else 'âœ—'}")
    print(f"ESSæ¯”ç‡: {ess_ratio:.1%} {'âœ“' if ess_ratio > 0.2 else 'âœ—'} (åŸºæº–: >20%)")
    
    if bias < 0.05 and coverage and ess_ratio > 0.2:
        print("\nğŸ‰ Stage 1 å®Œæˆï¼æ¬¡ã®Stage 2ï¼ˆDoubly Robustï¼‰ã«é€²ã‚€æº–å‚™ãŒã§ãã¾ã—ãŸã€‚")
    else:
        print("\nâš ï¸ Stage 1ã®åŸºæº–ã‚’æº€ãŸã—ã¦ã„ã¾ã›ã‚“ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´ãŒå¿…è¦ã§ã™ã€‚")
