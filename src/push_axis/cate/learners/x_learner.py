"""
X-Learnerå®Ÿè£…ï¼ˆCross-Fitting + ç›´äº¤åŒ–å¯¾å¿œï¼‰
Google/Meta/NASAãƒ¬ãƒ™ãƒ«ã®å³å¯†ãªå®Ÿè£…

å‚è€ƒæ–‡çŒ®ï¼š
- KÃ¼nzel et al. (2019) "Metalearners for estimating heterogeneous treatment effects using machine learning"
- Chernozhukov et al. (2018) "Double/debiased machine learning for treatment and structural parameters"
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LassoCV, RidgeCV
from sklearn.metrics import mean_squared_error
import warnings
warnings.filterwarnings('ignore')

class XLearner:
    """
    X-Learner (Cross-Learner) for CATE Estimation
    
    Google/Meta/NASAã§å®Ÿç”¨ã•ã‚Œã¦ã„ã‚‹é«˜ç²¾åº¦CATEæ¨å®šå™¨
    
    ç‰¹å¾´ï¼š
    - Cross-Fitting ã«ã‚ˆã‚‹éå­¦ç¿’ãƒã‚¤ã‚¢ã‚¹é™¤å»
    - æŸ”è»Ÿãªå­¦ç¿’å™¨é¸æŠï¼ˆRF, GBM, Lassoç­‰ï¼‰
    - å‚¾å‘ã‚¹ã‚³ã‚¢é‡ã¿ä»˜ãã§ã®æœ€é©çµåˆ
    - æ¨™æº–èª¤å·®æ¨å®šã¨CIè¨ˆç®—
    """
    
    def __init__(self, 
                 outcome_learner=None, 
                 effect_learner=None,
                 propensity_learner=None,
                 n_folds=5,
                 random_state=42):
        """
        Parameters:
        -----------
        outcome_learner : sklearn estimator
            ã‚¢ã‚¦ãƒˆã‚«ãƒ äºˆæ¸¬ç”¨å­¦ç¿’å™¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šRandomForestï¼‰
        effect_learner : sklearn estimator  
            åŠ¹æœäºˆæ¸¬ç”¨å­¦ç¿’å™¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šRandomForestï¼‰
        propensity_learner : sklearn estimator
            å‚¾å‘ã‚¹ã‚³ã‚¢äºˆæ¸¬ç”¨å­¦ç¿’å™¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šLogisticRegressionï¼‰
        n_folds : int
            Cross-fittingç”¨ã®åˆ†å‰²æ•°
        """
        self.outcome_learner = outcome_learner or RandomForestRegressor(
            n_estimators=100, max_depth=6, random_state=random_state
        )
        self.effect_learner = effect_learner or RandomForestRegressor(
            n_estimators=100, max_depth=4, random_state=random_state  
        )
        self.propensity_learner = propensity_learner or GradientBoostingRegressor(
            n_estimators=100, max_depth=3, random_state=random_state
        )
        self.n_folds = n_folds
        self.random_state = random_state
        
        # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¿å­˜ç”¨
        self.outcome_models_0 = []  # çµ±åˆ¶ç¾¤ç”¨
        self.outcome_models_1 = []  # å‡¦ç½®ç¾¤ç”¨  
        self.effect_models_0 = []   # çµ±åˆ¶ç¾¤ã§ã®åŠ¹æœäºˆæ¸¬
        self.effect_models_1 = []   # å‡¦ç½®ç¾¤ã§ã®åŠ¹æœäºˆæ¸¬
        self.propensity_models = []
        
        # Cross-fittingç”¨ã®äºˆæ¸¬çµæœä¿å­˜
        self.mu0_pred = None
        self.mu1_pred = None 
        self.tau0_pred = None
        self.tau1_pred = None
        self.e_pred = None
        
    def _split_data(self, X, y, treatment):
        """
        å‡¦ç½®ãƒ»çµ±åˆ¶ã‚°ãƒ«ãƒ¼ãƒ—ã§ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²
        """
        treated_idx = treatment == 1
        control_idx = treatment == 0
        
        X_treated = X[treated_idx]
        y_treated = y[treated_idx]
        X_control = X[control_idx]
        y_control = y[control_idx]
        
        return X_treated, y_treated, X_control, y_control
    
    def _fit_stage1_models(self, X_treated, y_treated, X_control, y_control):
        """
        Stage 1: åŸºæœ¬ã‚¢ã‚¦ãƒˆã‚«ãƒ ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
        Î¼â‚(x) = E[Y | A=1, X=x] and Î¼â‚€(x) = E[Y | A=0, X=x]
        """
        # å‡¦ç½®ç¾¤ã§ã®ã‚¢ã‚¦ãƒˆã‚«ãƒ äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
        mu1_model = self.outcome_learner.__class__(**self.outcome_learner.get_params())
        mu1_model.fit(X_treated, y_treated)
        
        # çµ±åˆ¶ç¾¤ã§ã®ã‚¢ã‚¦ãƒˆã‚«ãƒ äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«  
        mu0_model = self.outcome_learner.__class__(**self.outcome_learner.get_params())
        mu0_model.fit(X_control, y_control)
        
        return mu0_model, mu1_model
    
    def _compute_pseudo_outcomes(self, X_treated, y_treated, X_control, y_control, 
                                mu0_model, mu1_model):
        """
        Stage 2: ç–‘ä¼¼ã‚¢ã‚¦ãƒˆã‚«ãƒ ï¼ˆä»®æƒ³çš„ãªå€‹åˆ¥å‡¦ç½®åŠ¹æœï¼‰ã®è¨ˆç®—
        """
        # å‡¦ç½®ç¾¤ã§ã®ç–‘ä¼¼ã‚¢ã‚¦ãƒˆã‚«ãƒ : Dâ‚áµ¢ = Yáµ¢ - Î¼â‚€(Xáµ¢)
        # "å®Ÿéš›ã®çµæœ" - "ã‚‚ã—å‡¦ç½®ã‚’å—ã‘ãªã‹ã£ãŸå ´åˆã®äºˆæ¸¬çµæœ"
        mu0_pred_treated = mu0_model.predict(X_treated)
        D1 = y_treated - mu0_pred_treated
        
        # çµ±åˆ¶ç¾¤ã§ã®ç–‘ä¼¼ã‚¢ã‚¦ãƒˆã‚«ãƒ : Dâ‚€áµ¢ = Î¼â‚(Xáµ¢) - Yáµ¢  
        # "ã‚‚ã—å‡¦ç½®ã‚’å—ã‘ãŸå ´åˆã®äºˆæ¸¬çµæœ" - "å®Ÿéš›ã®çµæœ"
        mu1_pred_control = mu1_model.predict(X_control)
        D0 = mu1_pred_control - y_control
        
        return D1, D0
    
    def _fit_stage2_models(self, X_treated, D1, X_control, D0):
        """
        Stage 3: ç–‘ä¼¼ã‚¢ã‚¦ãƒˆã‚«ãƒ ã‹ã‚‰CATEãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
        """
        # å‡¦ç½®ç¾¤ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åŠ¹æœé–¢æ•°ã‚’å­¦ç¿’: Ï„â‚(x) = E[Dâ‚ | X=x]
        tau1_model = self.effect_learner.__class__(**self.effect_learner.get_params())
        tau1_model.fit(X_treated, D1)
        
        # çµ±åˆ¶ç¾¤ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åŠ¹æœé–¢æ•°ã‚’å­¦ç¿’: Ï„â‚€(x) = E[Dâ‚€ | X=x] 
        tau0_model = self.effect_learner.__class__(**self.effect_learner.get_params())
        tau0_model.fit(X_control, D0)
        
        return tau0_model, tau1_model
    
    def _fit_propensity_model(self, X, treatment):
        """
        å‚¾å‘ã‚¹ã‚³ã‚¢ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
        e(x) = P(A=1 | X=x)
        """
        propensity_model = self.propensity_learner.__class__(**self.propensity_learner.get_params())
        propensity_model.fit(X, treatment)
        return propensity_model
    
    def fit(self, X, y, treatment):
        """
        Cross-Fittingã‚’ä½¿ç”¨ã—ã¦X-Learnerã‚’å­¦ç¿’
        
        Parameters:
        -----------
        X : array-like, shape = [n_samples, n_features]
            ç‰¹å¾´é‡è¡Œåˆ—
        y : array-like, shape = [n_samples]
            ã‚¢ã‚¦ãƒˆã‚«ãƒ 
        treatment : array-like, shape = [n_samples]
            å‡¦ç½®å‰²ã‚Šå½“ã¦ï¼ˆ0 or 1ï¼‰
        """
        print("ğŸš€ X-Learnerå­¦ç¿’é–‹å§‹...")
        print(f"   ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X)}, ç‰¹å¾´é‡æ•°: {X.shape[1]}")
        print(f"   å‡¦ç½®ç¾¤: {treatment.sum()}, çµ±åˆ¶ç¾¤: {(1-treatment).sum()}")
        
        X = np.array(X)
        y = np.array(y) 
        treatment = np.array(treatment)
        n_samples = len(X)
        
        # Cross-fittingç”¨ã®äºˆæ¸¬çµæœã‚’åˆæœŸåŒ–
        self.mu0_pred = np.zeros(n_samples)
        self.mu1_pred = np.zeros(n_samples)
        self.tau0_pred = np.zeros(n_samples)
        self.tau1_pred = np.zeros(n_samples)
        self.e_pred = np.zeros(n_samples)
        
        # K-fold Cross-fitting
        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        
        fold_idx = 0
        for train_idx, val_idx in kf.split(X):
            print(f"   ğŸ“Š Fold {fold_idx + 1}/{self.n_folds} å‡¦ç†ä¸­...")
            
            # è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]
            treatment_train = treatment[train_idx]
            treatment_val = treatment[val_idx]
            
            # å‡¦ç½®ãƒ»çµ±åˆ¶ã‚°ãƒ«ãƒ¼ãƒ—ã§åˆ†å‰²
            X_treated, y_treated, X_control, y_control = self._split_data(
                X_train, y_train, treatment_train
            )
            
            # Stage 1: åŸºæœ¬ã‚¢ã‚¦ãƒˆã‚«ãƒ ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
            mu0_model, mu1_model = self._fit_stage1_models(
                X_treated, y_treated, X_control, y_control
            )
            
            # Stage 2: ç–‘ä¼¼ã‚¢ã‚¦ãƒˆã‚«ãƒ è¨ˆç®—
            D1, D0 = self._compute_pseudo_outcomes(
                X_treated, y_treated, X_control, y_control, mu0_model, mu1_model
            )
            
            # Stage 3: åŠ¹æœãƒ¢ãƒ‡ãƒ«å­¦ç¿’
            tau0_model, tau1_model = self._fit_stage2_models(
                X_treated, D1, X_control, D0
            )
            
            # å‚¾å‘ã‚¹ã‚³ã‚¢ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
            propensity_model = self._fit_propensity_model(X_train, treatment_train)
            
            # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬ï¼ˆCross-fittingã®æ ¸å¿ƒï¼‰
            self.mu0_pred[val_idx] = mu0_model.predict(X_val)
            self.mu1_pred[val_idx] = mu1_model.predict(X_val)
            self.tau0_pred[val_idx] = tau0_model.predict(X_val)
            self.tau1_pred[val_idx] = tau1_model.predict(X_val)
            self.e_pred[val_idx] = np.clip(propensity_model.predict(X_val), 0.01, 0.99)
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ï¼ˆæœ€çµ‚äºˆæ¸¬ç”¨ï¼‰
            self.outcome_models_0.append(mu0_model)
            self.outcome_models_1.append(mu1_model)  
            self.effect_models_0.append(tau0_model)
            self.effect_models_1.append(tau1_model)
            self.propensity_models.append(propensity_model)
            
            fold_idx += 1
        
        print("âœ… X-Learnerå­¦ç¿’å®Œäº†!")
        return self
    
    def predict_cate(self, X):
        """
        CATEäºˆæ¸¬ï¼ˆå‚¾å‘ã‚¹ã‚³ã‚¢é‡ã¿ä»˜ãçµåˆï¼‰
        
        Ï„(x) = g(x) * Ï„â‚(x) + (1 - g(x)) * Ï„â‚€(x)
        where g(x) = e(x) (å‚¾å‘ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã®é‡ã¿)
        """
        X = np.array(X)
        n_samples = len(X)
        
        # å„ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã®ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬â†’å¹³å‡åŒ–
        tau0_preds = np.zeros((self.n_folds, n_samples))
        tau1_preds = np.zeros((self.n_folds, n_samples))
        e_preds = np.zeros((self.n_folds, n_samples))
        
        for fold in range(self.n_folds):
            tau0_preds[fold] = self.effect_models_0[fold].predict(X)
            tau1_preds[fold] = self.effect_models_1[fold].predict(X) 
            e_preds[fold] = np.clip(self.propensity_models[fold].predict(X), 0.01, 0.99)
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å¹³å‡
        tau0_avg = tau0_preds.mean(axis=0)
        tau1_avg = tau1_preds.mean(axis=0)
        e_avg = e_preds.mean(axis=0)
        
        # å‚¾å‘ã‚¹ã‚³ã‚¢é‡ã¿ä»˜ãçµåˆ
        # g(x) = e(x): å‡¦ç½®ç¢ºç‡ãŒé«˜ã„â†’å‡¦ç½®ç¾¤ãƒ¢ãƒ‡ãƒ«é‡è¦–
        g_weights = e_avg
        cate_pred = g_weights * tau1_avg + (1 - g_weights) * tau0_avg
        
        return cate_pred
    
    def predict_ate(self):
        """
        å¹³å‡å‡¦ç½®åŠ¹æœï¼ˆATEï¼‰ã®æ¨å®š
        ATE = E[Ï„(X)] â‰ˆ (1/n) Î£áµ¢ Ï„(Xáµ¢)
        """
        if self.tau0_pred is None or self.tau1_pred is None:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å…ˆã«fit()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        
        # Cross-fittingçµæœã‹ã‚‰é‡ã¿ä»˜ãå¹³å‡ã§ATEè¨ˆç®—
        g_weights = self.e_pred
        cate_crossfit = g_weights * self.tau1_pred + (1 - g_weights) * self.tau0_pred
        ate_estimate = cate_crossfit.mean()
        
        return ate_estimate
    
    def compute_confidence_intervals(self, X, alpha=0.05):
        """
        CATEã®ä¿¡é ¼åŒºé–“ã‚’è¨ˆç®—
        
        Bootstrapã‚„Influence Function ã‚’ä½¿ç”¨ã—ãŸæ¨™æº–èª¤å·®æ¨å®š
        ï¼ˆç°¡æ˜“ç‰ˆï¼šãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰é–“åˆ†æ•£ã‚’ä½¿ç”¨ï¼‰
        """
        X = np.array(X)
        n_samples = len(X)
        
        # å„ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã§ã®äºˆæ¸¬ã‚’å–å¾—
        fold_predictions = []
        for fold in range(self.n_folds):
            tau0_pred = self.effect_models_0[fold].predict(X)
            tau1_pred = self.effect_models_1[fold].predict(X)
            e_pred = np.clip(self.propensity_models[fold].predict(X), 0.01, 0.99)
            
            cate_pred = e_pred * tau1_pred + (1 - e_pred) * tau0_pred
            fold_predictions.append(cate_pred)
        
        fold_predictions = np.array(fold_predictions)
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰é–“ã®æ¨™æº–èª¤å·®ã‚’è¨ˆç®—
        mean_pred = fold_predictions.mean(axis=0)
        std_pred = fold_predictions.std(axis=0, ddof=1)
        
        # tåˆ†å¸ƒã®è‡¨ç•Œå€¤ï¼ˆè‡ªç”±åº¦ = n_folds - 1ï¼‰
        from scipy.stats import t
        t_critical = t.ppf(1 - alpha/2, df=self.n_folds - 1)
        
        # ä¿¡é ¼åŒºé–“
        ci_lower = mean_pred - t_critical * std_pred / np.sqrt(self.n_folds)
        ci_upper = mean_pred + t_critical * std_pred / np.sqrt(self.n_folds)
        
        return mean_pred, ci_lower, ci_upper
    
    def evaluate_performance(self, X_test, true_cate):
        """
        äºˆæ¸¬æ€§èƒ½ã®è©•ä¾¡
        """
        cate_pred = self.predict_cate(X_test)
        
        # RMSE (Root Mean Square Error)
        rmse = np.sqrt(mean_squared_error(true_cate, cate_pred))
        
        # MAE (Mean Absolute Error)  
        mae = np.mean(np.abs(true_cate - cate_pred))
        
        # Correlation
        correlation = np.corrcoef(true_cate, cate_pred)[0, 1]
        
        # RÂ² Score
        ss_res = np.sum((true_cate - cate_pred) ** 2)
        ss_tot = np.sum((true_cate - np.mean(true_cate)) ** 2)
        r2_score = 1 - (ss_res / ss_tot)
        
        return {
            'RMSE': rmse,
            'MAE': mae, 
            'Correlation': correlation,
            'R2_Score': r2_score
        }

# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆé–¢æ•°
def test_x_learner():
    """
    X-Learnerã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    """
    print("ğŸ§ª X-Learner ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ...")
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆå‰ã®ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ï¼‰
    from src.push_axis.cate.utils.data_preprocessing import generate_sample_data
    train_data, test_data, _ = generate_sample_data()
    
    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’åˆ†é›¢
    feature_cols = ['age', 'gender', 'purchase_count', 'avg_purchase_amount', 'app_usage', 'region']
    feature_cols = train_data["X"].columns.tolist()
    feature_cols = train_data["X"].columns.tolist()
    X_train = train_data["X"][feature_cols].values
    y_train = train_data["Y"].values
    treatment_train = train_data["T"].values
    
    X_test = test_data["X"][feature_cols].values
    true_cate_test = test_data["tau"].values
    
    # X-Learnerå­¦ç¿’
    xl = XLearner(n_folds=5, random_state=42)
    xl.fit(X_train, y_train, treatment_train)
    
    # ATEæ¨å®š
    ate_estimate = xl.predict_ate()
    true_ate = train_data["tau"].mean()
    print(f"\nğŸ“Š ATEæ¨å®šçµæœ:")
    print(f"   çœŸã®ATE: {true_ate:.4f}")
    print(f"   æ¨å®šATE: {ate_estimate:.4f}")
    print(f"   èª¤å·®: {abs(ate_estimate - true_ate):.4f}")
    
    # CATEäºˆæ¸¬ã¨CI
    cate_pred, ci_lower, ci_upper = xl.compute_confidence_intervals(X_test[:100])
    
    # æ€§èƒ½è©•ä¾¡
    performance = xl.evaluate_performance(X_test, true_cate_test)
    print(f"\nğŸ“ˆ CATEäºˆæ¸¬æ€§èƒ½:")
    for metric, value in performance.items():
        print(f"   {metric}: {value:.4f}")
    
    return xl, performance

if __name__ == "__main__":
    xl_model, results = test_x_learner()
