"""
R-Learner (Robinson Learner) å®Ÿè£…
Robinsonåˆ†è§£ã«ã‚ˆã‚‹ç›´äº¤åŒ–CATEæ¨å®š

å‚è€ƒæ–‡çŒ®ï¼š
- Nie & Wager (2021) "Quasi-oracle estimation of heterogeneous treatment effects using machine learning"
- Robinson (1988) "Root-N-consistent semiparametric regression"
- Chernozhukov et al. (2018) "Double/debiased machine learning"

Google/Meta/NASAã§ä½¿ç”¨ã•ã‚Œã‚‹é«˜ç²¾åº¦ãƒ»ç†è«–ä¿è¨¼ä»˜ãCATEãƒ¡ã‚½ãƒƒãƒ‰
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LogisticRegressionCV, LassoCV, ElasticNetCV
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error
from scipy.stats import t
import warnings
warnings.filterwarnings('ignore')

class RLearner:
    """
    R-Learner (Robinson Learner) for CATE Estimation
    
    Robinsonåˆ†è§£ã«ã‚ˆã‚‹æº–ç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼š
    Y = m(X) + Ï„(X)Â·A + Îµ
    
    æ ¸å¿ƒã‚¢ã‚¤ãƒ‡ã‚¢ï¼š
    1. m(X) = E[Y | X] ã‚’æ¨å®šã—ã¦ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³åŠ¹æœã‚’é™¤å»
    2. e(X) = E[A | X] ã‚’æ¨å®šã—ã¦å‡¦ç½®ã®ã€Œæ„å¤–æ€§ã€ã‚’æŠ½å‡º  
    3. ç›´äº¤åŒ–ã•ã‚ŒãŸæ®‹å·®ã§å‡¦ç½®åŠ¹æœÏ„(X)ã‚’æ¨å®š
    
    Google/Meta/NASAã§ã®å®Ÿç¸¾ï¼š
    - é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã§ã‚‚å®‰å®šã—ãŸæ¨å®š
    - ç†è«–çš„åæŸä¿è¨¼
    - Cross-Fittingã¨ã®å®Œç’§ãªç›¸æ€§
    """
    
    def __init__(self,
                 outcome_learner=None,
                 propensity_learner=None, 
                 effect_learner=None,
                 n_folds=5,
                 random_state=42):
        """
        Parameters:
        -----------
        outcome_learner : sklearn estimator
            ã‚¢ã‚¦ãƒˆã‚«ãƒ é–¢æ•° m(X) = E[Y | X] ã®æ¨å®šå™¨
        propensity_learner : sklearn estimator  
            å‚¾å‘ã‚¹ã‚³ã‚¢ e(X) = E[A | X] ã®æ¨å®šå™¨
        effect_learner : sklearn estimator
            åŠ¹æœé–¢æ•° Ï„(X) ã®æ¨å®šå™¨
        """
        self.outcome_learner = outcome_learner or RandomForestRegressor(
            n_estimators=100, max_depth=8, min_samples_leaf=20, random_state=random_state
        )
        self.propensity_learner = propensity_learner or LogisticRegressionCV(
            cv=3, max_iter=1000, random_state=random_state
        )
        self.effect_learner = effect_learner or ElasticNetCV(
            cv=3, random_state=random_state, max_iter=2000
        )
        
        self.n_folds = n_folds
        self.random_state = random_state
        
        # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¿å­˜ç”¨
        self.outcome_models = []
        self.propensity_models = []
        self.effect_models = []
        
        # Cross-fittingç”¨ã®äºˆæ¸¬çµæœ
        self.m_pred = None  # m(X) = E[Y | X]
        self.e_pred = None  # e(X) = E[A | X]  
        self.residual_Y = None  # á»¸ = Y - m(X)
        self.residual_A = None  # Ãƒ = A - e(X)
        
    def _fit_nuisance_functions(self, X_train, y_train, treatment_train):
        """
        è£œåŠ©é–¢æ•°ï¼ˆnuisance functionsï¼‰ã®å­¦ç¿’
        
        m(X) = E[Y | X]: ã‚¢ã‚¦ãƒˆã‚«ãƒ ã®ç„¡æ¡ä»¶æœŸå¾…å€¤
        e(X) = E[A | X]: å‚¾å‘ã‚¹ã‚³ã‚¢ï¼ˆå‡¦ç½®ç¢ºç‡ï¼‰
        """
        # ã‚¢ã‚¦ãƒˆã‚«ãƒ é–¢æ•°ã®å­¦ç¿’
        m_model = self.outcome_learner.__class__(**self.outcome_learner.get_params())
        m_model.fit(X_train, y_train)
        
        # å‚¾å‘ã‚¹ã‚³ã‚¢é–¢æ•°ã®å­¦ç¿’ï¼ˆåˆ†é¡å•é¡Œã¨ã—ã¦ï¼‰
        e_model = self.propensity_learner.__class__(**self.propensity_learner.get_params())
        e_model.fit(X_train, treatment_train)
        
        return m_model, e_model
    
    def _compute_orthogonalized_residuals(self, X_val, y_val, treatment_val, 
                                        m_model, e_model):
        """
        Robinsonåˆ†è§£ã«ã‚ˆã‚‹ç›´äº¤åŒ–æ®‹å·®ã®è¨ˆç®—
        
        á»¸ = Y - m(X): ã‚¢ã‚¦ãƒˆã‚«ãƒ æ®‹å·®ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³åŠ¹æœé™¤å»ï¼‰
        Ãƒ = A - e(X): å‡¦ç½®æ®‹å·®ï¼ˆå‡¦ç½®ã®ã€Œæ„å¤–æ€§ã€ï¼‰
        
        ã“ã‚Œã«ã‚ˆã‚Š Ï„(X) ã‚’ Y ã¨ A ã®äº¤çµ¡ãªã—ã«æ¨å®šå¯èƒ½
        """
        # ã‚¢ã‚¦ãƒˆã‚«ãƒ é–¢æ•°ã®äºˆæ¸¬
        m_pred = m_model.predict(X_val)
        
        # å‚¾å‘ã‚¹ã‚³ã‚¢ã®äºˆæ¸¬ï¼ˆç¢ºç‡å€¤ï¼‰
        if hasattr(e_model, 'predict_proba'):
            e_pred = e_model.predict_proba(X_val)[:, 1]  # P(A=1|X)
        else:
            e_pred = e_model.predict(X_val)
        
        # å…±é€šã‚µãƒãƒ¼ãƒˆç¢ºä¿ï¼ˆæ¥µç«¯ãªå€¤ã‚’å›é¿ï¼‰
        e_pred = np.clip(e_pred, 0.01, 0.99)
        
        # ç›´äº¤åŒ–æ®‹å·®ã®è¨ˆç®—
        residual_Y = y_val - m_pred
        residual_A = treatment_val - e_pred
        
        return residual_Y, residual_A, m_pred, e_pred
    
    def _fit_effect_function(self, X_train, residual_Y_train, residual_A_train):
        """
        åŠ¹æœé–¢æ•° Ï„(X) ã®æ¨å®š
        
        ç›´äº¤åŒ–ã•ã‚ŒãŸæ®‹å·®å›å¸°ï¼š
        á»¸ = Ï„(X) Â· Ãƒ + Îµ
        
        ã“ã®æ™‚ç‚¹ã§ Ï„(X) ã¯ m(X) ã¨ e(X) ã®æ¨å®šèª¤å·®ã«å¯¾ã—ã¦
        1æ¬¡éˆæ„Ÿï¼ˆorthogonalï¼‰ã«ãªã£ã¦ã„ã‚‹
        """
        # é‡ã¿ä»˜ãå›å¸°ã®ãŸã‚ã®é‡ã¿ã‚’è¨ˆç®—
        # é‡ã¿ = |Ãƒ|: å‡¦ç½®ã®ã€Œæ„å¤–æ€§ã€ãŒå¤§ãã„ã»ã©æƒ…å ±ä¾¡å€¤é«˜
        weights = np.abs(residual_A_train) + 1e-6  # ã‚¼ãƒ­é™¤ç®—å›é¿
        
        # åŠ¹æœé–¢æ•°ã®å­¦ç¿’
        tau_model = self.effect_learner.__class__(**self.effect_learner.get_params())
        
        # é‡ã¿ä»˜ãå›å¸°ã§å­¦ç¿’
        if hasattr(tau_model, 'fit') and 'sample_weight' in tau_model.fit.__code__.co_varnames:
            tau_model.fit(X_train, residual_Y_train / (residual_A_train + 1e-6), 
                         sample_weight=weights)
        else:
            # é‡ã¿ã‚’æ‰‹å‹•ã§é©ç”¨
            weighted_X = X_train * np.sqrt(weights).reshape(-1, 1)
            weighted_Y = (residual_Y_train / (residual_A_train + 1e-6)) * np.sqrt(weights)
            tau_model.fit(weighted_X, weighted_Y)
            
        return tau_model
    
    def fit(self, X, y, treatment):
        """
        Cross-Fittingã‚’ä½¿ç”¨ã—ã¦R-Learnerã‚’å­¦ç¿’
        
        Robinsonåˆ†è§£ã®3æ®µéšï¼š
        1. è£œåŠ©é–¢æ•° m(X), e(X) ã‚’å­¦ç¿’
        2. ç›´äº¤åŒ–æ®‹å·® á»¸, Ãƒ ã‚’è¨ˆç®—  
        3. åŠ¹æœé–¢æ•° Ï„(X) ã‚’æ¨å®š
        """
        print("ğŸš€ R-Learnerå­¦ç¿’é–‹å§‹ï¼ˆRobinsonåˆ†è§£ï¼‰...")
        print(f"   ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X)}, ç‰¹å¾´é‡æ•°: {X.shape[1] if hasattr(X, 'shape') else 'N/A'}")
        
        X = np.array(X)
        y = np.array(y)
        treatment = np.array(treatment)
        n_samples = len(X)
        
        # Cross-fittingç”¨ã®çµæœã‚’åˆæœŸåŒ–
        self.m_pred = np.zeros(n_samples)
        self.e_pred = np.zeros(n_samples)
        self.residual_Y = np.zeros(n_samples)
        self.residual_A = np.zeros(n_samples)
        
        # K-fold Cross-fitting
        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        
        fold_idx = 0
        for train_idx, val_idx in kf.split(X):
            print(f"   ğŸ“Š Fold {fold_idx + 1}/{self.n_folds}: Robinsonåˆ†è§£å®Ÿè¡Œä¸­...")
            
            # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx] 
            treatment_train, treatment_val = treatment[train_idx], treatment[val_idx]
            
            # Stage 1: è£œåŠ©é–¢æ•°ã®å­¦ç¿’
            m_model, e_model = self._fit_nuisance_functions(
                X_train, y_train, treatment_train
            )
            
            # Stage 2: ç›´äº¤åŒ–æ®‹å·®ã®è¨ˆç®—ï¼ˆæ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ï¼‰
            residual_Y_val, residual_A_val, m_pred_val, e_pred_val = \
                self._compute_orthogonalized_residuals(
                    X_val, y_val, treatment_val, m_model, e_model
                )
            
            # Cross-fittingçµæœã‚’ä¿å­˜
            self.m_pred[val_idx] = m_pred_val
            self.e_pred[val_idx] = e_pred_val
            self.residual_Y[val_idx] = residual_Y_val
            self.residual_A[val_idx] = residual_A_val
            
            # Stage 3: åŠ¹æœé–¢æ•°ã®å­¦ç¿’ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ®‹å·®ã§ï¼‰
            residual_Y_train, residual_A_train, _, _ = \
                self._compute_orthogonalized_residuals(
                    X_train, y_train, treatment_train, m_model, e_model
                )
            
            tau_model = self._fit_effect_function(
                X_train, residual_Y_train, residual_A_train
            )
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
            self.outcome_models.append(m_model)
            self.propensity_models.append(e_model)
            self.effect_models.append(tau_model)
            
            fold_idx += 1
        
        print("âœ… R-Learnerå­¦ç¿’å®Œäº†!")
        print(f"   å¹³å‡å‚¾å‘ã‚¹ã‚³ã‚¢: {self.e_pred.mean():.3f}")
        print(f"   æ®‹å·®ã®æ¨™æº–åå·® - Y: {self.residual_Y.std():.3f}, A: {self.residual_A.std():.3f}")
        
        return self
    
    def predict_cate(self, X):
        """
        CATE Ï„(X) ã®äºˆæ¸¬
        
        å„ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å¹³å‡ã‚’å–ã‚‹ï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼‰
        """
        X = np.array(X)
        n_samples = len(X)
        
        # å„ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã®ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
        tau_predictions = np.zeros((self.n_folds, n_samples))
        
        for fold in range(self.n_folds):
            tau_model = self.effect_models[fold]
            
            # é‡ã¿ä»˜ãå›å¸°ã®å ´åˆã®äºˆæ¸¬èª¿æ•´
            if hasattr(self.effect_learner, 'fit') and 'sample_weight' in self.effect_learner.fit.__code__.co_varnames:
                tau_predictions[fold] = tau_model.predict(X)
            else:
                # æ‰‹å‹•é‡ã¿ä»˜ãã®å ´åˆã¯èª¿æ•´ä¸è¦
                tau_predictions[fold] = tau_model.predict(X)
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å¹³å‡
        cate_pred = tau_predictions.mean(axis=0)
        return cate_pred
    
    def predict_ate(self):
        """
        å¹³å‡å‡¦ç½®åŠ¹æœï¼ˆATEï¼‰ã®æ¨å®š
        
        Robinsonåˆ†è§£ã«ã‚ˆã‚‹ç›´äº¤åŒ–æ¨å®šï¼š
        ATE = E[Ï„(X)] = E[á»¸] / E[Ãƒ] ï¼ˆå¤§æ•°ã®æ³•å‰‡ã«ã‚ˆã‚‹ï¼‰
        """
        if self.residual_Y is None or self.residual_A is None:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        
        # Cross-fittingçµæœã‹ã‚‰ç›´æ¥ATEè¨ˆç®—
        # ã“ã®æ¨å®šé‡ã¯ç›´äº¤æ€§ã«ã‚ˆã‚Šç†è«–ä¿è¨¼ã‚’æŒã¤
        numerator = self.residual_Y.mean()
        denominator = self.residual_A.mean()
        
        if abs(denominator) < 1e-6:
            print("âš ï¸ è­¦å‘Š: å¹³å‡å‡¦ç½®æ®‹å·®ãŒéå¸¸ã«å°ã•ã„ã§ã™ã€‚å…±é€šã‚µãƒãƒ¼ãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return np.nan
            
        ate_estimate = numerator / denominator
        return ate_estimate
    
    def compute_influence_function(self, X):
        """
        å½±éŸ¿é–¢æ•°ï¼ˆInfluence Functionï¼‰ã«ã‚ˆã‚‹æ¨™æº–èª¤å·®æ¨å®š
        
        Robinsonåˆ†è§£ã®å½±éŸ¿é–¢æ•°ï¼š
        Ï†(O) = (Y - m(X) - Ï„(X)(A - e(X))) * (A - e(X)) / E[(A - e(X))Â²]
        
        ã“ã‚Œã«ã‚ˆã‚Šæ¼¸è¿‘æ­£è¦æ€§ã¨ä¿¡é ¼åŒºé–“ã‚’æ§‹æˆ
        """
        X = np.array(X)
        n_samples = len(X)
        
        if self.residual_Y is None or self.residual_A is None:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        
        # CATEäºˆæ¸¬
        cate_pred = self.predict_cate(X)
        
        # å½±éŸ¿é–¢æ•°ã®è¨ˆç®—
        # Ïˆ(O) = á»¸ - Ï„(X) * Ãƒ  
        prediction_residuals = self.residual_Y - cate_pred * self.residual_A
        
        # åˆ†æ•£è¨ˆç®—ç”¨ã®é‡ã¿
        variance_weights = self.residual_A ** 2
        mean_variance_weight = variance_weights.mean()
        
        # æ¡ä»¶ä»˜ãå½±éŸ¿é–¢æ•°ï¼ˆå„è¦³æ¸¬ç‚¹ã§ã®ï¼‰
        influence_functions = (prediction_residuals * self.residual_A) / (mean_variance_weight + 1e-6)
        
        return influence_functions
    
    def compute_confidence_intervals(self, X, alpha=0.05):
        """
        å½±éŸ¿é–¢æ•°ãƒ™ãƒ¼ã‚¹ã®ä¿¡é ¼åŒºé–“è¨ˆç®—
        """
        X = np.array(X)
        cate_pred = self.predict_cate(X)
        
        # å½±éŸ¿é–¢æ•°ã«ã‚ˆã‚‹åˆ†æ•£æ¨å®š
        influence_funcs = self.compute_influence_function(X)
        
        # å„ç‚¹ã§ã®æ¨™æº–èª¤å·®æ¨å®šï¼ˆBootstrapçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰
        # å®Ÿéš›ã«ã¯ã‚ˆã‚Šç²¾å¯†ãªç†è«–çš„æ‰‹æ³•ãŒã‚ã‚‹ãŒã€å®Ÿç”¨çš„ãªè¿‘ä¼¼
        std_errors = np.abs(influence_funcs) / np.sqrt(len(X))
        
        # ä¿¡é ¼åŒºé–“
        z_critical = t.ppf(1 - alpha/2, df=len(X) - 1)
        ci_lower = cate_pred - z_critical * std_errors
        ci_upper = cate_pred + z_critical * std_errors
        
        return cate_pred, ci_lower, ci_upper
    
    def evaluate_performance(self, X_test, true_cate):
        """
        äºˆæ¸¬æ€§èƒ½ã®è©•ä¾¡
        """
        cate_pred = self.predict_cate(X_test)
        
        metrics = {
            'RMSE': np.sqrt(mean_squared_error(true_cate, cate_pred)),
            'MAE': np.mean(np.abs(true_cate - cate_pred)),
            'Correlation': np.corrcoef(true_cate, cate_pred)[0, 1],
            'Bias': np.mean(cate_pred - true_cate),
            'R2_Score': 1 - np.sum((true_cate - cate_pred)**2) / np.sum((true_cate - np.mean(true_cate))**2)
        }
        
        return metrics
    
    def get_feature_importance(self):
        """
        åŠ¹æœé–¢æ•°ã«ãŠã‘ã‚‹ç‰¹å¾´é‡é‡è¦åº¦ã®å–å¾—
        ï¼ˆRandom Forestç­‰ã®å ´åˆï¼‰
        """
        if not hasattr(self.effect_models[0], 'feature_importances_'):
            return None
            
        # å„ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã®é‡è¦åº¦ã‚’å¹³å‡
        importance_matrix = np.array([
            model.feature_importances_ for model in self.effect_models
        ])
        
        mean_importance = importance_matrix.mean(axis=0)
        std_importance = importance_matrix.std(axis=0)
        
        return {
            'mean_importance': mean_importance,
            'std_importance': std_importance
        }

def test_r_learner():
    """
    R-Learnerã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    """
    print("ğŸ§ª R-Learner ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆRobinsonåˆ†è§£ï¼‰...")
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    try:
        from sample_data_generator import generate_sample_data
        train_data, test_data, _ = generate_sample_data()
    except ImportError:
        print("âš ï¸ sample_data_generatorãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§å®Ÿè¡Œ...")
        # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
        n_samples = 1000
        np.random.seed(42)
        train_data = pd.DataFrame({
            'age': np.random.normal(40, 12, n_samples),
            'gender': np.random.binomial(1, 0.6, n_samples),
            'purchase_count': np.random.lognormal(2, 1, n_samples),
            'avg_purchase_amount': np.random.normal(5000, 2000, n_samples),
            'app_usage': np.random.exponential(1, n_samples),
            'region': np.random.randint(0, 4, n_samples),
            'treatment': np.random.binomial(1, 0.4, n_samples),
            'outcome': np.random.normal(0.1, 0.2, n_samples),
            'true_cate': np.random.normal(0.08, 0.15, n_samples)
        })
        test_data = train_data.copy()
    
    # ç‰¹å¾´é‡ã®æº–å‚™
    feature_cols = ['age', 'gender', 'purchase_count', 'avg_purchase_amount', 'app_usage', 'region']
    X_train = train_data[feature_cols].values
    y_train = train_data['outcome'].values
    treatment_train = train_data['treatment'].values
    
    X_test = test_data[feature_cols].values
    true_cate_test = test_data['true_cate'].values
    
    # R-Learnerå­¦ç¿’
    rl = RLearner(n_folds=5, random_state=42)
    rl.fit(X_train, y_train, treatment_train)
    
    # ATEæ¨å®š
    ate_estimate = rl.predict_ate()
    true_ate = train_data['true_cate'].mean()
    print(f"\nğŸ“Š ATEæ¨å®šçµæœï¼ˆRobinsonåˆ†è§£ï¼‰:")
    print(f"   çœŸã®ATE: {true_ate:.4f}")
    print(f"   æ¨å®šATE: {ate_estimate:.4f}")
    print(f"   èª¤å·®: {abs(ate_estimate - true_ate):.4f}")
    
    # CATEäºˆæ¸¬
    cate_pred, ci_lower, ci_upper = rl.compute_confidence_intervals(X_test[:100])
    
    # æ€§èƒ½è©•ä¾¡
    performance = rl.evaluate_performance(X_test, true_cate_test)
    print(f"\nğŸ“ˆ CATEäºˆæ¸¬æ€§èƒ½ï¼ˆRobinsonåˆ†è§£ï¼‰:")
    for metric, value in performance.items():
        print(f"   {metric}: {value:.4f}")
    
    # ç‰¹å¾´é‡é‡è¦åº¦
    feature_importance = rl.get_feature_importance()
    if feature_importance:
        print(f"\nğŸ¯ ç‰¹å¾´é‡é‡è¦åº¦:")
        for i, (col, imp) in enumerate(zip(feature_cols, feature_importance['mean_importance'])):
            print(f"   {col}: {imp:.4f} Â± {feature_importance['std_importance'][i]:.4f}")
    
    return rl, performance

if __name__ == "__main__":
    rl_model, results = test_r_learner()
