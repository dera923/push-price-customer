# libs/causal/dr_ate.py

import numpy as np
import pandas as pd
from typing import Tuple, Dict, Optional
from dataclasses import dataclass

@dataclass
class DREstimatorResult:
    """DRæ¨å®šçµæœã‚’æ ¼ç´ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    ate: float
    se: float  
    ci_lower: float
    ci_upper: float
    influence_function: np.ndarray
    n_treated: int
    n_control: int
    
class DoubleRobustATE:
    """
    ãƒ€ãƒ–ãƒ«ãƒ­ãƒã‚¹ãƒˆå¹³å‡å‡¦ç½®åŠ¹æœæ¨å®šå™¨
    Google/Metaãƒ¬ãƒ™ãƒ«ã®å®Ÿè£…
    """
    
    def __init__(self, 
                 outcome_model=None,
                 propensity_model=None,
                 trim_threshold: float = 0.01):
        """
        Args:
            outcome_model: çµæœäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ï¼ˆsklearnäº’æ›ï¼‰
            propensity_model: å‚¾å‘ã‚¹ã‚³ã‚¢ãƒ¢ãƒ‡ãƒ«ï¼ˆsklearnäº’æ›ï¼‰
            trim_threshold: å‚¾å‘ã‚¹ã‚³ã‚¢ã®ãƒˆãƒªãƒŸãƒ³ã‚°é–¾å€¤
        """
        self.outcome_model = outcome_model
        self.propensity_model = propensity_model
        self.trim_threshold = trim_threshold
        
    def fit(self, X: np.ndarray, T: np.ndarray, Y: np.ndarray) -> 'DoubleRobustATE':
        """
        ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
        
        Args:
            X: å…±å¤‰é‡è¡Œåˆ— (n_samples, n_features)
            T: å‡¦ç½®ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ (n_samples,)
            Y: çµæœå¤‰æ•° (n_samples,)
        """
        # 1. å‚¾å‘ã‚¹ã‚³ã‚¢ã®å­¦ç¿’
        print("ğŸ“Š å‚¾å‘ã‚¹ã‚³ã‚¢ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ä¸­...")
        self.propensity_model.fit(X, T)
        self.e_hat = self.propensity_model.predict_proba(X)[:, 1]
        
        # ãƒˆãƒªãƒŸãƒ³ã‚°ï¼ˆæ¥µç«¯ãªå‚¾å‘ã‚¹ã‚³ã‚¢ã‚’åˆ¶é™ï¼‰
        self.e_hat = np.clip(self.e_hat, self.trim_threshold, 1 - self.trim_threshold)
        
        # 2. çµæœãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ï¼ˆå‡¦ç½®ç¾¤ã¨çµ±åˆ¶ç¾¤ã§åˆ¥ã€…ã«ï¼‰
        print("ğŸ“ˆ çµæœãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ä¸­...")
        
        # å‡¦ç½®ç¾¤ãƒ¢ãƒ‡ãƒ«
        X_treated = X[T == 1]
        Y_treated = Y[T == 1]
        self.outcome_model_1 = self._clone_model(self.outcome_model)
        self.outcome_model_1.fit(X_treated, Y_treated)
        
        # çµ±åˆ¶ç¾¤ãƒ¢ãƒ‡ãƒ«  
        X_control = X[T == 0]
        Y_control = Y[T == 0]
        self.outcome_model_0 = self._clone_model(self.outcome_model)
        self.outcome_model_0.fit(X_control, Y_control)
        
        # äºˆæ¸¬å€¤
        self.mu_1_hat = self.outcome_model_1.predict(X)
        self.mu_0_hat = self.outcome_model_0.predict(X)
        
        # 3. DRæ¨å®šé‡ã®è¨ˆç®—
        self.ate_result = self._compute_dr_ate(X, T, Y)
        
        return self
    
    def _compute_dr_ate(self, X: np.ndarray, T: np.ndarray, Y: np.ndarray) -> DREstimatorResult:
        """
        ãƒ€ãƒ–ãƒ«ãƒ­ãƒã‚¹ãƒˆæ¨å®šé‡ã®è¨ˆç®—
        
        Googleã®åºƒå‘ŠåŠ¹æœæ¸¬å®šã§ä½¿ã‚ã‚Œã‚‹å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³
        """
        n = len(Y)
        
        # å½±éŸ¿é–¢æ•°ã®è¨ˆç®—ï¼ˆã“ã‚ŒãŒçµ±è¨ˆç†è«–ã®æ ¸å¿ƒï¼‰
        psi = np.zeros(n)
        
        for i in range(n):
            # å›å¸°èª¿æ•´é …
            regression_term = self.mu_1_hat[i] - self.mu_0_hat[i]
            
            # IPWè£œæ­£é …ï¼ˆå‡¦ç½®ç¾¤ï¼‰
            if T[i] == 1:
                ipw_correction = (Y[i] - self.mu_1_hat[i]) / self.e_hat[i]
            else:
                # IPWè£œæ­£é …ï¼ˆçµ±åˆ¶ç¾¤ï¼‰
                ipw_correction = -(Y[i] - self.mu_0_hat[i]) / (1 - self.e_hat[i])
            
            psi[i] = regression_term + ipw_correction
        
        # ATEç‚¹æ¨å®š
        ate = np.mean(psi)
        
        # æ¨™æº–èª¤å·®ï¼ˆã‚µãƒ³ãƒ‰ã‚¤ãƒƒãƒåˆ†æ•£æ¨å®šï¼‰
        # Metaã§ã¯ã“ã®è¨ˆç®—ã«HC3ï¼ˆheteroskedasticity-consistentï¼‰ã‚’ä½¿ç”¨
        var_psi = np.var(psi, ddof=1)
        se = np.sqrt(var_psi / n)
        
        # 95%ä¿¡é ¼åŒºé–“
        ci_lower = ate - 1.96 * se
        ci_upper = ate + 1.96 * se
        
        return DREstimatorResult(
            ate=ate,
            se=se,
            ci_lower=ci_lower,
            ci_upper=ci_upper,
            influence_function=psi,
            n_treated=np.sum(T),
            n_control=np.sum(1 - T)
        )
    
    def _clone_model(self, model):
        """ãƒ¢ãƒ‡ãƒ«ã®ã‚¯ãƒ­ãƒ¼ãƒ³ä½œæˆ"""
        from sklearn.base import clone
        return clone(model)
    
    def get_diagnostics(self) -> Dict:
        """
        è¨ºæ–­çµ±è¨ˆé‡ã®è¨ˆç®—
        NASAã®ãƒŸãƒƒã‚·ãƒ§ãƒ³ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãªè§£æã§ä½¿ç”¨ã•ã‚Œã‚‹å“è³ªãƒã‚§ãƒƒã‚¯
        """
        # å‚¾å‘ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒ
        ps_stats = {
            'min': np.min(self.e_hat),
            'max': np.max(self.e_hat),
            'mean': np.mean(self.e_hat),
            'extreme_low': np.mean(self.e_hat < 0.1),
            'extreme_high': np.mean(self.e_hat > 0.9)
        }
        
        # æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºï¼ˆEffective Sample Sizeï¼‰
        weights_treated = 1 / self.e_hat
        weights_control = 1 / (1 - self.e_hat)
        
        ess_treated = np.sum(weights_treated[self.T == 1])**2 / \
                     np.sum(weights_treated[self.T == 1]**2)
        ess_control = np.sum(weights_control[self.T == 0])**2 / \
                     np.sum(weights_control[self.T == 0]**2)
        
        return {
            'propensity_score': ps_stats,
            'ess_treated': ess_treated,
            'ess_control': ess_control,
            'max_weight': max(np.max(weights_treated), np.max(weights_control))
        }
