# libs/causal/dr_ate.py

import numpy as np
import pandas as pd
from typing import Tuple, Dict, Optional
from dataclasses import dataclass
from scikit-learn.base import clone  # scikit-learnã‚’æ˜ç¤ºçš„ã«ä½¿ç”¨

@dataclass
class DREstimatorResult:
    """DRæ¨å®šçµæœã‚’æ ¼ç´ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    ate: float
    se: float  
    ci_lower: float
    ci_upper: float
    influence_function: np.ndarray
    n_treated: int
    n_control: int
    propensity_scores: np.ndarray
    outcome_predictions_1: np.ndarray
    outcome_predictions_0: np.ndarray
    
class DoubleRobustATE:
    """
    ãƒ€ãƒ–ãƒ«ãƒ­ãƒã‚¹ãƒˆå¹³å‡å‡¦ç½®åŠ¹æœæ¨å®šå™¨
    
    å‚è€ƒå®Ÿè£…:
    - Google Ads ã® Causal Impact Framework
    - Meta ã® Adaptive Experimentation Platform
    - Microsoft ã® EconML ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
    """
    
    def __init__(self, 
                 outcome_model=None,
                 propensity_model=None,
                 trim_threshold: float = 0.01,
                 verbose: bool = True):
        """
        Args:
            outcome_model: çµæœäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ï¼ˆscikit-learnäº’æ›ï¼‰
            propensity_model: å‚¾å‘ã‚¹ã‚³ã‚¢ãƒ¢ãƒ‡ãƒ«ï¼ˆscikit-learnäº’æ›ï¼‰
            trim_threshold: å‚¾å‘ã‚¹ã‚³ã‚¢ã®ãƒˆãƒªãƒŸãƒ³ã‚°é–¾å€¤
            verbose: è©³ç´°å‡ºåŠ›ãƒ•ãƒ©ã‚°
        """
        self.outcome_model = outcome_model
        self.propensity_model = propensity_model
        self.trim_threshold = trim_threshold
        self.verbose = verbose
        
        # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
        self.outcome_model_1 = None
        self.outcome_model_0 = None
        self.e_hat = None
        self.mu_1_hat = None
        self.mu_0_hat = None
        
    def fit(self, X: np.ndarray, T: np.ndarray, Y: np.ndarray) -> 'DoubleRobustATE':
        """
        ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
        
        Args:
            X: å…±å¤‰é‡è¡Œåˆ— (n_samples, n_features)
            T: å‡¦ç½®ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ (n_samples,)
            Y: çµæœå¤‰æ•° (n_samples,)
        """
        if self.verbose:
            print("\n" + "="*60)
            print("ğŸ”¬ ãƒ€ãƒ–ãƒ«ãƒ­ãƒã‚¹ãƒˆæ¨å®šã®å®Ÿè¡Œ")
            print("="*60)
        
        # å…¥åŠ›æ¤œè¨¼
        self._validate_inputs(X, T, Y)
        
        # 1. å‚¾å‘ã‚¹ã‚³ã‚¢ã®å­¦ç¿’
        if self.verbose:
            print("\nğŸ“Š Step 1: å‚¾å‘ã‚¹ã‚³ã‚¢ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’")
            print("-"*40)
        
        self.propensity_model.fit(X, T)
        
        # å‚¾å‘ã‚¹ã‚³ã‚¢ã®äºˆæ¸¬
        if hasattr(self.propensity_model, 'predict_proba'):
            self.e_hat = self.propensity_model.predict_proba(X)[:, 1]
        else:
            # predict_probaãŒãªã„å ´åˆï¼ˆä¾‹ï¼šç·šå½¢å›å¸°ï¼‰
            self.e_hat = self.propensity_model.predict(X)
            self.e_hat = np.clip(self.e_hat, 0, 1)
        
        # ãƒˆãƒªãƒŸãƒ³ã‚°ï¼ˆæ¥µç«¯ãªå‚¾å‘ã‚¹ã‚³ã‚¢ã‚’åˆ¶é™ï¼‰
        original_e_hat = self.e_hat.copy()
        self.e_hat = np.clip(self.e_hat, self.trim_threshold, 1 - self.trim_threshold)
        
        if self.verbose:
            n_trimmed = np.sum((original_e_hat < self.trim_threshold) | 
                              (original_e_hat > 1 - self.trim_threshold))
            print(f"   å‚¾å‘ã‚¹ã‚³ã‚¢ç¯„å›²: [{self.e_hat.min():.3f}, {self.e_hat.max():.3f}]")
            print(f"   ãƒˆãƒªãƒŸãƒ³ã‚°ã•ã‚ŒãŸè¦³æ¸¬: {n_trimmed} ({n_trimmed/len(T)*100:.1f}%)")
        
        # 2. çµæœãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ï¼ˆå‡¦ç½®ç¾¤ã¨çµ±åˆ¶ç¾¤ã§åˆ¥ã€…ã«ï¼‰
        if self.verbose:
            print("\nğŸ“ˆ Step 2: çµæœãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’")
            print("-"*40)
        
        # å‡¦ç½®ç¾¤ãƒ¢ãƒ‡ãƒ«
        X_treated = X[T == 1]
        Y_treated = Y[T == 1]
        self.outcome_model_1 = clone(self.outcome_model)
        self.outcome_model_1.fit(X_treated, Y_treated)
        
        # çµ±åˆ¶ç¾¤ãƒ¢ãƒ‡ãƒ«  
        X_control = X[T == 0]
        Y_control = Y[T == 0]
        self.outcome_model_0 = clone(self.outcome_model)
        self.outcome_model_0.fit(X_control, Y_control)
        
        # äºˆæ¸¬å€¤
        self.mu_1_hat = self.outcome_model_1.predict(X)
        self.mu_0_hat = self.outcome_model_0.predict(X)
        
        if self.verbose:
            print(f"   å‡¦ç½®ç¾¤ãƒ¢ãƒ‡ãƒ«: {len(Y_treated)} ã‚µãƒ³ãƒ—ãƒ«ã§å­¦ç¿’")
            print(f"   çµ±åˆ¶ç¾¤ãƒ¢ãƒ‡ãƒ«: {len(Y_control)} ã‚µãƒ³ãƒ—ãƒ«ã§å­¦ç¿’")
        
        # 3. DRæ¨å®šé‡ã®è¨ˆç®—
        if self.verbose:
            print("\nğŸ¯ Step 3: ãƒ€ãƒ–ãƒ«ãƒ­ãƒã‚¹ãƒˆæ¨å®šé‡ã®è¨ˆç®—")
            print("-"*40)
        
        self.ate_result = self._compute_dr_ate(X, T, Y)
        
        if self.verbose:
            self._print_results()
        
        return self
    
    def _validate_inputs(self, X, T, Y):
        """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼"""
        if len(X) != len(T) or len(X) != len(Y):
            raise ValueError("X, T, Y must have the same length")
        
        if not np.all(np.isin(T, [0, 1])):
            raise ValueError("T must be binary (0 or 1)")
        
        if np.any(np.isnan(X)) or np.any(np.isnan(T)) or np.any(np.isnan(Y)):
            raise ValueError("Input contains NaN values")
    
    def _compute_dr_ate(self, X: np.ndarray, T: np.ndarray, Y: np.ndarray) -> DREstimatorResult:
        """
        ãƒ€ãƒ–ãƒ«ãƒ­ãƒã‚¹ãƒˆæ¨å®šé‡ã®è¨ˆç®—
        
        ç†è«–èƒŒæ™¯:
        - Robins et al. (1994) ã®Augmented IPWæ¨å®šé‡
        - Bang & Robins (2005) ã®ãƒ€ãƒ–ãƒ«ãƒ­ãƒã‚¹ãƒˆæ¨å®š
        """
        n = len(Y)
        
        # å½±éŸ¿é–¢æ•°ã®è¨ˆç®—
        psi = np.zeros(n)
        
        for i in range(n):
            # å›å¸°èª¿æ•´é …ï¼ˆOutcome regression componentï¼‰
            regression_term = self.mu_1_hat[i] - self.mu_0_hat[i]
            
            # IPWè£œæ­£é …ï¼ˆPropensity score weighting componentï¼‰
            if T[i] == 1:
                # å‡¦ç½®ç¾¤ã®è£œæ­£
                ipw_correction = (Y[i] - self.mu_1_hat[i]) / self.e_hat[i]
            else:
                # çµ±åˆ¶ç¾¤ã®è£œæ­£
                ipw_correction = -(Y[i] - self.mu_0_hat[i]) / (1 - self.e_hat[i])
            
            # ãƒ€ãƒ–ãƒ«ãƒ­ãƒã‚¹ãƒˆæ¨å®šé‡ã®å½±éŸ¿é–¢æ•°
            psi[i] = regression_term + ipw_correction
        
        # ATEç‚¹æ¨å®š
        ate = np.mean(psi)
        
        # æ¨™æº–èª¤å·®ã®è¨ˆç®—ï¼ˆã‚µãƒ³ãƒ‰ã‚¤ãƒƒãƒåˆ†æ•£æ¨å®šï¼‰
        var_psi = np.var(psi, ddof=1)  # ä¸ååˆ†æ•£
        se = np.sqrt(var_psi / n)
        
        # 95%ä¿¡é ¼åŒºé–“
        z_alpha = 1.96  # æ­£è¦åˆ†å¸ƒã®97.5%ç‚¹
        ci_lower = ate - z_alpha * se
        ci_upper = ate + z_alpha * se
        
        return DREstimatorResult(
            ate=ate,
            se=se,
            ci_lower=ci_lower,
            ci_upper=ci_upper,
            influence_function=psi,
            n_treated=np.sum(T),
            n_control=np.sum(1 - T),
            propensity_scores=self.e_hat,
            outcome_predictions_1=self.mu_1_hat,
            outcome_predictions_0=self.mu_0_hat
        )
    
    def _print_results(self):
        """çµæœã®è¡¨ç¤º"""
        result = self.ate_result
        
        print(f"\nğŸ¯ æ¨å®šçµæœ:")
        print(f"   ATE = {result.ate:,.2f} å††")
        print(f"   SE  = {result.se:,.2f} å††")
        print(f"   95% CI = [{result.ci_lower:,.2f}, {result.ci_upper:,.2f}]")
        
        # çµ±è¨ˆçš„æœ‰æ„æ€§ã®åˆ¤å®š
        is_significant = result.ci_lower > 0 or result.ci_upper < 0
        if is_significant:
            if result.ate > 0:
                print(f"   âœ… çµ±è¨ˆçš„ã«æœ‰æ„ãªãƒã‚¸ãƒ†ã‚£ãƒ–åŠ¹æœ")
            else:
                print(f"   âš ï¸ çµ±è¨ˆçš„ã«æœ‰æ„ãªãƒã‚¬ãƒ†ã‚£ãƒ–åŠ¹æœ")
        else:
            print(f"   âŒ çµ±è¨ˆçš„ã«æœ‰æ„ã§ã¯ãªã„")
    
    def get_diagnostics(self) -> Dict:
        """
        è¨ºæ–­çµ±è¨ˆé‡ã®è¨ˆç®—
        """
        if self.ate_result is None:
            raise ValueError("Model must be fitted first")
        
        # å‚¾å‘ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒçµ±è¨ˆé‡
        ps_stats = {
            'min': np.min(self.e_hat),
            'max': np.max(self.e_hat),
            'mean': np.mean(self.e_hat),
            'std': np.std(self.e_hat),
            'extreme_low': np.mean(self.e_hat < 0.1),
            'extreme_high': np.mean(self.e_hat > 0.9)
        }
        
        # æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºï¼ˆEffective Sample Sizeï¼‰
        # Kish (1965) ã®å®šç¾©ã«åŸºã¥ã
        weights_treated = 1 / self.e_hat
        weights_control = 1 / (1 - self.e_hat)
        
        # å‡¦ç½®ç¾¤ã®ESS
        treated_mask = self.ate_result.n_treated
        ess_treated = np.sum(weights_treated)**2 / np.sum(weights_treated**2)
        
        # çµ±åˆ¶ç¾¤ã®ESS
        ess_control = np.sum(weights_control)**2 / np.sum(weights_control**2)
        
        return {
            'propensity_score': ps_stats,
            'ess_treated': ess_treated,
            'ess_control': ess_control,
            'ess_ratio': min(ess_treated, ess_control) / len(self.e_hat),
            'max_weight': max(np.max(weights_treated), np.max(weights_control))
        }
